1
00:00:02,880 --> 00:00:06,770
[Music]

2
00:00:06,770 --> 00:00:10,860
welcome everyone I am Jeremiah and this

3
00:00:10,860 --> 00:00:15,120
is tensorflow in production I'm excited

4
00:00:15,120 --> 00:00:16,440
that you're all here because that means

5
00:00:16,440 --> 00:00:18,750
you're excited about production and that

6
00:00:18,750 --> 00:00:20,580
means you're building things that people

7
00:00:20,580 --> 00:00:24,330
actually use so our talk today has three

8
00:00:24,330 --> 00:00:27,180
parts I want to start by quickly drawing

9
00:00:27,180 --> 00:00:28,890
a thread that kind of connects all of

10
00:00:28,890 --> 00:00:32,369
them and the first thread is the origin

11
00:00:32,369 --> 00:00:35,430
of these projects these projects really

12
00:00:35,430 --> 00:00:36,840
come from our teams that are on the

13
00:00:36,840 --> 00:00:40,020
front line of machine learning so these

14
00:00:40,020 --> 00:00:41,579
are real problems that we've come across

15
00:00:41,579 --> 00:00:43,620
doing machine learning at Google's scale

16
00:00:43,620 --> 00:00:46,170
and these are the real solutions that

17
00:00:46,170 --> 00:00:53,250
let us do machine learning at Google the

18
00:00:53,250 --> 00:00:54,690
second thing I want to talk about is

19
00:00:54,690 --> 00:00:58,920
this observation if we look at software

20
00:00:58,920 --> 00:01:01,289
engineering over the years we see this

21
00:01:01,289 --> 00:01:04,530
growth as we discover new tools as we

22
00:01:04,530 --> 00:01:06,720
discover best practices we're really

23
00:01:06,720 --> 00:01:09,060
getting more effective at doing machine

24
00:01:09,060 --> 00:01:12,090
or doing software engineering and we're

25
00:01:12,090 --> 00:01:15,210
getting more efficient we're seeing the

26
00:01:15,210 --> 00:01:17,670
same kind of growth on the machine

27
00:01:17,670 --> 00:01:19,890
learning side right we're discovering

28
00:01:19,890 --> 00:01:22,770
new best practices and new tools the

29
00:01:22,770 --> 00:01:25,799
catch is that this growth is maybe 10 or

30
00:01:25,799 --> 00:01:29,369
15 years behind software engineering and

31
00:01:29,369 --> 00:01:31,229
we're also rediscovering a lot of the

32
00:01:31,229 --> 00:01:33,030
same things that exist in software

33
00:01:33,030 --> 00:01:35,369
engineering but in a machine learning

34
00:01:35,369 --> 00:01:37,310
context so we're doing things like

35
00:01:37,310 --> 00:01:40,439
discovering version control for machine

36
00:01:40,439 --> 00:01:43,380
learning or continuous integration for

37
00:01:43,380 --> 00:01:47,520
machine learning so I think it's worth

38
00:01:47,520 --> 00:01:49,829
keeping that in mind as we move through

39
00:01:49,829 --> 00:01:51,600
the talks the first one up is going to

40
00:01:51,600 --> 00:01:54,270
be tensorflow hub and this is something

41
00:01:54,270 --> 00:01:57,540
that lets you share reusable pieces of

42
00:01:57,540 --> 00:01:58,950
machine learning much the same way we

43
00:01:58,950 --> 00:02:01,110
share code then we'll talk a little bit

44
00:02:01,110 --> 00:02:03,210
about deploying machine learning models

45
00:02:03,210 --> 00:02:05,969
with tensorflow serving and we'll finish

46
00:02:05,969 --> 00:02:08,700
up with tensorflow extended which wraps

47
00:02:08,700 --> 00:02:10,649
a lot of these things together in a

48
00:02:10,649 --> 00:02:13,750
platform to increase your velocity

49
00:02:13,750 --> 00:02:16,210
as a machine-learning practitioner so

50
00:02:16,210 --> 00:02:18,310
with that I'd like to hand it over to

51
00:02:18,310 --> 00:02:23,550
Andrew to talk about tensorflow hub

52
00:02:23,550 --> 00:02:26,800
thanks Jeremiah hi everybody I'm Andrew

53
00:02:26,800 --> 00:02:29,500
Gasper ovitch and I'd like to talk to

54
00:02:29,500 --> 00:02:31,060
you a little bit about tensorflow hub

55
00:02:31,060 --> 00:02:34,120
which is a new library that's designed

56
00:02:34,120 --> 00:02:37,920
to bring reusability to machine learning

57
00:02:37,920 --> 00:02:43,330
so software repositories have been a

58
00:02:43,330 --> 00:02:46,330
real benefit to developer productivity

59
00:02:46,330 --> 00:02:49,420
over the past 10 or 15 years and are

60
00:02:49,420 --> 00:02:49,840
great

61
00:02:49,840 --> 00:02:52,780
first of all because when you're writing

62
00:02:52,780 --> 00:02:55,989
something new if you have a repository

63
00:02:55,989 --> 00:02:58,600
you think oh maybe I'll check whether

64
00:02:58,600 --> 00:03:00,489
there's something that already exists

65
00:03:00,489 --> 00:03:02,680
and reuse that rather than starting from

66
00:03:02,680 --> 00:03:06,370
scratch but a second thing that happens

67
00:03:06,370 --> 00:03:10,300
is you start thinking maybe I'll write

68
00:03:10,300 --> 00:03:12,459
my code in a way that's specifically

69
00:03:12,459 --> 00:03:15,370
designed for reuse which is great

70
00:03:15,370 --> 00:03:17,739
because it makes your code more modular

71
00:03:17,739 --> 00:03:20,230
but it also has a potential to benefit a

72
00:03:20,230 --> 00:03:24,420
whole community if you share that code

73
00:03:24,420 --> 00:03:27,580
what we are doing with tensorflow hub is

74
00:03:27,580 --> 00:03:30,430
bringing that idea of a repository to

75
00:03:30,430 --> 00:03:35,049
machine learning in this case tensorflow

76
00:03:35,049 --> 00:03:36,880
hub is designed so that you can create

77
00:03:36,880 --> 00:03:41,640
share and reuse components of ml models

78
00:03:41,640 --> 00:03:46,680
and if you think about it it's even more

79
00:03:46,680 --> 00:03:49,360
important to have a repository for

80
00:03:49,360 --> 00:03:51,940
machine learning even more so than

81
00:03:51,940 --> 00:03:54,910
software development because in the case

82
00:03:54,910 --> 00:03:56,769
of machine learning not only are you

83
00:03:56,769 --> 00:04:00,180
reusing the algorithm and the expertise

84
00:04:00,180 --> 00:04:02,890
but you're also reusing potentially

85
00:04:02,890 --> 00:04:04,930
enormous amount of compute power that

86
00:04:04,930 --> 00:04:07,209
went into training the model and all of

87
00:04:07,209 --> 00:04:11,079
the training data as well so all four of

88
00:04:11,079 --> 00:04:13,420
those the algorithm the training data

89
00:04:13,420 --> 00:04:16,269
the compute and the expertise all go

90
00:04:16,269 --> 00:04:20,049
into a module which is shareable

91
00:04:20,049 --> 00:04:22,780
tensorflow hub and then you can import

92
00:04:22,780 --> 00:04:26,139
those into your model

93
00:04:26,139 --> 00:04:31,080
and those models those modules are

94
00:04:31,080 --> 00:04:33,490
pre-trained so they have the weights and

95
00:04:33,490 --> 00:04:39,159
the tensor flow graph inside and unlike

96
00:04:39,159 --> 00:04:40,770
a model they're designed to be

97
00:04:40,770 --> 00:04:43,629
composable which means that you can put

98
00:04:43,629 --> 00:04:45,610
them together like building blocks and

99
00:04:45,610 --> 00:04:48,159
add to your own stuff on top they're

100
00:04:48,159 --> 00:04:50,199
reusable which means that they have

101
00:04:50,199 --> 00:04:52,180
common signatures so that you can swap

102
00:04:52,180 --> 00:04:55,000
one for another and they're retraining

103
00:04:55,000 --> 00:04:56,530
Bowl which means that you can actually

104
00:04:56,530 --> 00:04:59,379
back propagate through a module that

105
00:04:59,379 --> 00:05:03,819
you've inserted into your graph so let's

106
00:05:03,819 --> 00:05:06,129
take a quick look at an example in this

107
00:05:06,129 --> 00:05:08,740
case we'll do a little bit of image

108
00:05:08,740 --> 00:05:11,500
classification and say that we want to

109
00:05:11,500 --> 00:05:15,009
make an app to classify rabbit breeds

110
00:05:15,009 --> 00:05:18,279
from photos but we only have a few

111
00:05:18,279 --> 00:05:21,520
hundred example photos probably not

112
00:05:21,520 --> 00:05:23,560
enough to build a whole image classifier

113
00:05:23,560 --> 00:05:26,770
from scratch but what we could do is

114
00:05:26,770 --> 00:05:30,629
start from a general-purpose model and

115
00:05:30,629 --> 00:05:34,479
we could take the reusable part of it

116
00:05:34,479 --> 00:05:36,460
the architecture and the weights their

117
00:05:36,460 --> 00:05:40,180
takeoff the classification and then we

118
00:05:40,180 --> 00:05:42,659
could add our own classifier on top and

119
00:05:42,659 --> 00:05:46,330
train it with our own examples we'll

120
00:05:46,330 --> 00:05:49,240
keep that reusable part fixed and we'll

121
00:05:49,240 --> 00:05:55,210
train our own classifier on top so if

122
00:05:55,210 --> 00:05:57,400
you're using tensorflow hub you start at

123
00:05:57,400 --> 00:06:00,969
tensorflow org / hub where you can find

124
00:06:00,969 --> 00:06:03,539
a whole bunch of newly released

125
00:06:03,539 --> 00:06:06,550
state-of-the-art research-oriented and

126
00:06:06,550 --> 00:06:10,839
the well-known image modules some of

127
00:06:10,839 --> 00:06:15,430
them are include classification and some

128
00:06:15,430 --> 00:06:16,689
of them chop off the class

129
00:06:16,689 --> 00:06:19,569
classification layers and just output

130
00:06:19,569 --> 00:06:21,849
feature vectors so that's what we want

131
00:06:21,849 --> 00:06:23,620
in our own case in this case because

132
00:06:23,620 --> 00:06:27,060
we're going to add classification on top

133
00:06:27,060 --> 00:06:30,849
so maybe we'll choose NASA net which is

134
00:06:30,849 --> 00:06:34,569
a an image module that was created by a

135
00:06:34,569 --> 00:06:37,479
neural architecture search so we'll

136
00:06:37,479 --> 00:06:39,760
choose NASA net a the large version

137
00:06:39,760 --> 00:06:42,490
with the feature vectors so we just

138
00:06:42,490 --> 00:06:47,080
paste the URL for the module into our TF

139
00:06:47,080 --> 00:06:49,660
hub code and then we're ready to use

140
00:06:49,660 --> 00:06:51,940
that module just like a function in

141
00:06:51,940 --> 00:06:54,430
between the module gets downloaded and

142
00:06:54,430 --> 00:06:57,070
instantiate it into your graph so all

143
00:06:57,070 --> 00:06:59,260
you have to do is get those feature

144
00:06:59,260 --> 00:07:01,810
vectors add your own classification on

145
00:07:01,810 --> 00:07:07,510
top and output the the new categories so

146
00:07:07,510 --> 00:07:09,300
specifically what we're doing is

147
00:07:09,300 --> 00:07:11,950
training just the classification part

148
00:07:11,950 --> 00:07:13,960
while keeping all of the modules way

149
00:07:13,960 --> 00:07:16,480
it's fixed but the great thing about

150
00:07:16,480 --> 00:07:20,410
reusing a module is that you get all of

151
00:07:20,410 --> 00:07:24,430
the training and compute that's gone

152
00:07:24,430 --> 00:07:27,940
into that reusable portion so in the

153
00:07:27,940 --> 00:07:31,090
case of nazma it was over 62,000 GPU

154
00:07:31,090 --> 00:07:33,190
hours that went into finding the

155
00:07:33,190 --> 00:07:35,830
architecture and training the model plus

156
00:07:35,830 --> 00:07:39,670
all of the expertise the testing and the

157
00:07:39,670 --> 00:07:42,340
research that went into nazma you're

158
00:07:42,340 --> 00:07:45,010
reusing all of that in that one line of

159
00:07:45,010 --> 00:07:50,830
code and as I mentioned before those

160
00:07:50,830 --> 00:07:53,980
modules are trainable so if you have

161
00:07:53,980 --> 00:07:56,770
enough data you can do fine tuning with

162
00:07:56,770 --> 00:07:59,350
the module if you set that trainable

163
00:07:59,350 --> 00:08:01,870
parameter to true and you select that

164
00:08:01,870 --> 00:08:04,930
you want to use the training graph what

165
00:08:04,930 --> 00:08:07,060
you'll end up doing is training the

166
00:08:07,060 --> 00:08:09,100
entire thing along with your

167
00:08:09,100 --> 00:08:13,180
classification the caveat being that of

168
00:08:13,180 --> 00:08:14,980
course you have to lower the learning

169
00:08:14,980 --> 00:08:16,870
rate so that you don't ruin the weights

170
00:08:16,870 --> 00:08:19,660
inside the module but if you have enough

171
00:08:19,660 --> 00:08:21,250
training data it's something that you

172
00:08:21,250 --> 00:08:25,480
can do to get even better accuracy and

173
00:08:25,480 --> 00:08:29,020
in general we have lots of image modules

174
00:08:29,020 --> 00:08:31,240
on TF hub we have ones that are straight

175
00:08:31,240 --> 00:08:34,060
out of research papers like NASA net we

176
00:08:34,060 --> 00:08:35,680
have ones that are great for production

177
00:08:35,680 --> 00:08:38,470
even once made for on device usage like

178
00:08:38,470 --> 00:08:40,570
mobile net plus all of the industry

179
00:08:40,570 --> 00:08:42,430
standard ones that people are familiar

180
00:08:42,430 --> 00:08:47,020
with like inception and ResNet so let's

181
00:08:47,020 --> 00:08:49,180
look at one more example in this case

182
00:08:49,180 --> 00:08:50,950
doing a little bit of text

183
00:08:50,950 --> 00:08:52,339
classification

184
00:08:52,339 --> 00:08:56,480
we'll do look at some restaurant reviews

185
00:08:56,480 --> 00:08:58,790
and decide whether they're positive or

186
00:08:58,790 --> 00:09:03,079
negative sentiment and one of the great

187
00:09:03,079 --> 00:09:06,079
things about TF hub is that all of those

188
00:09:06,079 --> 00:09:07,999
modules because they're tensor flow

189
00:09:07,999 --> 00:09:09,999
graphs you can include things like

190
00:09:09,999 --> 00:09:13,009
pre-processing so the text modules that

191
00:09:13,009 --> 00:09:16,160
are available on TF hub take whole

192
00:09:16,160 --> 00:09:18,110
sentences and phrases not just

193
00:09:18,110 --> 00:09:20,180
individual words because they have all

194
00:09:20,180 --> 00:09:23,470
of the tokenization and pre-processing

195
00:09:23,470 --> 00:09:28,069
stored in the graph itself so we'll use

196
00:09:28,069 --> 00:09:30,860
one of those and basically the same idea

197
00:09:30,860 --> 00:09:32,240
we're going to select a sentence

198
00:09:32,240 --> 00:09:34,490
embedding module we'll add our own

199
00:09:34,490 --> 00:09:37,759
classification on top and we'll train it

200
00:09:37,759 --> 00:09:40,100
with our own data but we'll keep the

201
00:09:40,100 --> 00:09:45,769
module itself fixed and just like before

202
00:09:45,769 --> 00:09:47,689
we'll start by going to tensorflow org

203
00:09:47,689 --> 00:09:50,809
slash hub and take a look at the text

204
00:09:50,809 --> 00:09:53,180
modules that are available in this case

205
00:09:53,180 --> 00:09:54,829
maybe we'll choose the universal

206
00:09:54,829 --> 00:09:57,589
sentence encoder which is just recently

207
00:09:57,589 --> 00:10:00,769
released based on a research paper from

208
00:10:00,769 --> 00:10:03,139
last month the idea is that it was

209
00:10:03,139 --> 00:10:05,209
trained on a variety of tasks and is

210
00:10:05,209 --> 00:10:07,759
specifically meant to support using it

211
00:10:07,759 --> 00:10:09,769
with a variety of tasks and it also

212
00:10:09,769 --> 00:10:12,410
takes just a very small amount of

213
00:10:12,410 --> 00:10:16,550
training data to use it in your model

214
00:10:16,550 --> 00:10:20,029
which is perfect for our example case so

215
00:10:20,029 --> 00:10:21,920
we'll use that universal sentence

216
00:10:21,920 --> 00:10:24,199
encoder and just like before we'll paste

217
00:10:24,199 --> 00:10:27,649
the URL into our code the difference

218
00:10:27,649 --> 00:10:30,740
here is we're using it with a text

219
00:10:30,740 --> 00:10:34,189
embedding column that way we can feed it

220
00:10:34,189 --> 00:10:36,740
into one of the high-level tensorflow

221
00:10:36,740 --> 00:10:38,720
estimators in this case the DNN

222
00:10:38,720 --> 00:10:41,329
classifier but you could also use that

223
00:10:41,329 --> 00:10:44,120
module it like I showed in the earlier

224
00:10:44,120 --> 00:10:48,499
example calling it just as a function if

225
00:10:48,499 --> 00:10:50,300
you are using the text and embedding

226
00:10:50,300 --> 00:10:53,059
column that also just like any other

227
00:10:53,059 --> 00:10:56,660
example can be trained as well and just

228
00:10:56,660 --> 00:10:58,790
like any other example it's something

229
00:10:58,790 --> 00:11:00,589
that you can do with a lower learning

230
00:11:00,589 --> 00:11:03,290
rate if you have a lot of training data

231
00:11:03,290 --> 00:11:07,269
and it may give you better accuracy

232
00:11:07,269 --> 00:11:13,310
and so we have a lot of text modules

233
00:11:13,310 --> 00:11:16,519
available on TF we actually just added

234
00:11:16,519 --> 00:11:19,880
three new languages to the nn/lm modules

235
00:11:19,880 --> 00:11:23,420
Chinese Korean and Indonesian those are

236
00:11:23,420 --> 00:11:26,389
all trained on G news training data and

237
00:11:26,389 --> 00:11:29,509
we also have a really great module

238
00:11:29,509 --> 00:11:31,519
called Elmo from some recent research

239
00:11:31,519 --> 00:11:36,470
which understands words in context and

240
00:11:36,470 --> 00:11:37,970
of course the universal a sentence

241
00:11:37,970 --> 00:11:42,410
encoder as I talked about so just to

242
00:11:42,410 --> 00:11:44,540
show you for a minute some of those URLs

243
00:11:44,540 --> 00:11:47,720
that we've been looking at maybe we'll

244
00:11:47,720 --> 00:11:49,600
take apart the pieces here

245
00:11:49,600 --> 00:11:54,769
TF hub dev is our new source for Google

246
00:11:54,769 --> 00:11:58,040
and selected partner published modules

247
00:11:58,040 --> 00:12:01,190
in this case this is Google that's the

248
00:12:01,190 --> 00:12:03,380
publisher and new universal sentence

249
00:12:03,380 --> 00:12:07,579
encoder is the name of the module the

250
00:12:07,579 --> 00:12:10,069
one at the end is a version number so

251
00:12:10,069 --> 00:12:14,449
tensorflow hub considers modules to be

252
00:12:14,449 --> 00:12:17,930
immutable and so the version number is

253
00:12:17,930 --> 00:12:21,319
there so that if you're you know doing

254
00:12:21,319 --> 00:12:23,480
one training run and then another you

255
00:12:23,480 --> 00:12:25,519
don't have a situation where the the

256
00:12:25,519 --> 00:12:28,519
module change changes unexpectedly so

257
00:12:28,519 --> 00:12:32,029
all modules on TF abductive are version

258
00:12:32,029 --> 00:12:36,829
that way and one of the nice things

259
00:12:36,829 --> 00:12:38,750
about those URLs if you paste them into

260
00:12:38,750 --> 00:12:41,180
a browser you get the module

261
00:12:41,180 --> 00:12:44,269
documentation the idea being that maybe

262
00:12:44,269 --> 00:12:47,300
you read a new paper you see oh there's

263
00:12:47,300 --> 00:12:50,720
a URL for TF hub module and it you paste

264
00:12:50,720 --> 00:12:52,220
it into your browser you see the

265
00:12:52,220 --> 00:12:54,709
documentation you paste it into some

266
00:12:54,709 --> 00:12:57,529
code and in one line you're able to use

267
00:12:57,529 --> 00:12:59,930
that module and try out the new research

268
00:12:59,930 --> 00:13:04,670
and speaking of the universal encoder

269
00:13:04,670 --> 00:13:08,509
the team just released a new light

270
00:13:08,509 --> 00:13:11,149
version which is a much smaller size

271
00:13:11,149 --> 00:13:13,339
it's about 25 megabytes and it's

272
00:13:13,339 --> 00:13:16,730
specifically designed for cases where

273
00:13:16,730 --> 00:13:19,100
the full text module wouldn't

274
00:13:19,100 --> 00:13:20,540
for doing things like on device

275
00:13:20,540 --> 00:13:25,340
classification also today we released a

276
00:13:25,340 --> 00:13:28,280
new module from deep mind this one you

277
00:13:28,280 --> 00:13:31,250
can feed in video and it will classify

278
00:13:31,250 --> 00:13:35,360
and detect the actions in that video so

279
00:13:35,360 --> 00:13:37,790
in this case it correctly guesses the

280
00:13:37,790 --> 00:13:45,050
video is of people playing cricket and

281
00:13:45,050 --> 00:13:47,800
of course we also have a number of other

282
00:13:47,800 --> 00:13:50,330
interesting modules there's a generative

283
00:13:50,330 --> 00:13:52,820
image module which is trained on Salemme

284
00:13:52,820 --> 00:13:54,680
it has a progressive

285
00:13:54,680 --> 00:13:57,860
gann inside and also the deep local

286
00:13:57,860 --> 00:14:00,800
features module which can identify the

287
00:14:00,800 --> 00:14:03,500
key points of landmark images those are

288
00:14:03,500 --> 00:14:08,060
all available now on TFM and last but

289
00:14:08,060 --> 00:14:09,530
not least I wanted to mention that we

290
00:14:09,530 --> 00:14:12,350
just announced our support for

291
00:14:12,350 --> 00:14:15,470
tensorflow j/s so using the tensorflow

292
00:14:15,470 --> 00:14:18,290
j/s converter you can directly convert a

293
00:14:18,290 --> 00:14:20,720
TF hub module into a format that can be

294
00:14:20,720 --> 00:14:23,240
used on the web it's a really simple

295
00:14:23,240 --> 00:14:26,510
integration to be able to take a module

296
00:14:26,510 --> 00:14:29,180
and use it in the web browser with

297
00:14:29,180 --> 00:14:31,400
sensor flow jas and we're really excited

298
00:14:31,400 --> 00:14:36,950
to see what you build with it so just to

299
00:14:36,950 --> 00:14:38,060
summarize tensorflow

300
00:14:38,060 --> 00:14:41,300
hub is designed to be a starting point

301
00:14:41,300 --> 00:14:43,820
for reusable machine learning and the

302
00:14:43,820 --> 00:14:45,590
idea is just like with a software

303
00:14:45,590 --> 00:14:49,270
repository before you start from scratch

304
00:14:49,270 --> 00:14:52,430
check out what's available on tensorflow

305
00:14:52,430 --> 00:14:55,910
hub and you may find that it's better to

306
00:14:55,910 --> 00:14:58,250
start with a module and import that into

307
00:14:58,250 --> 00:15:01,220
your model rather than starting the task

308
00:15:01,220 --> 00:15:03,590
completely from scratch we have a lot of

309
00:15:03,590 --> 00:15:05,660
modules available and we're adding more

310
00:15:05,660 --> 00:15:06,380
all the time

311
00:15:06,380 --> 00:15:09,260
and we're really excited to see what you

312
00:15:09,260 --> 00:15:13,040
built so thanks next up is jeremiah to

313
00:15:13,040 --> 00:15:19,070
talk about TF serving

314
00:15:19,070 --> 00:15:22,560
all right thank you Andrew so next

315
00:15:22,560 --> 00:15:24,780
tensorflow serving this is going to be

316
00:15:24,780 --> 00:15:28,740
how we deploy modules our deploy models

317
00:15:28,740 --> 00:15:30,840
just to get a sense for where this falls

318
00:15:30,840 --> 00:15:33,660
in the machine learning process right we

319
00:15:33,660 --> 00:15:36,780
start with our data we use tensorflow to

320
00:15:36,780 --> 00:15:39,690
train a model in the output our artifact

321
00:15:39,690 --> 00:15:41,790
there are these models right these are

322
00:15:41,790 --> 00:15:44,100
saved models it's a graphical

323
00:15:44,100 --> 00:15:47,970
representation of the data flow and once

324
00:15:47,970 --> 00:15:49,620
we have those we want to share them with

325
00:15:49,620 --> 00:15:50,250
the world

326
00:15:50,250 --> 00:15:52,170
that's where tensorflow serving comes in

327
00:15:52,170 --> 00:15:54,990
it's this big orange box so this is

328
00:15:54,990 --> 00:15:56,640
something that takes our models and

329
00:15:56,640 --> 00:15:58,410
exposes them to the world through a

330
00:15:58,410 --> 00:16:01,040
service so clients can make requests

331
00:16:01,040 --> 00:16:04,500
tensorflow serving will take them run

332
00:16:04,500 --> 00:16:07,410
the inference run the model come up with

333
00:16:07,410 --> 00:16:11,990
an answer and return that in a response

334
00:16:11,990 --> 00:16:14,850
so tensorflow serving is actually the

335
00:16:14,850 --> 00:16:16,920
libraries and binary is you need to do

336
00:16:16,920 --> 00:16:18,440
this to do this production-grade

337
00:16:18,440 --> 00:16:21,840
inference overtrained tensorflow models

338
00:16:21,840 --> 00:16:25,620
it's written in c++ and supports things

339
00:16:25,620 --> 00:16:28,380
like G RPC and plays nicely with

340
00:16:28,380 --> 00:16:32,820
kubernetes so to do this well it has a

341
00:16:32,820 --> 00:16:35,670
couple of features the first and most

342
00:16:35,670 --> 00:16:38,390
important is it supports multiple models

343
00:16:38,390 --> 00:16:42,540
so on one tensor flow model server you

344
00:16:42,540 --> 00:16:44,790
can load multiple models right and just

345
00:16:44,790 --> 00:16:47,250
like most folks probably wouldn't push a

346
00:16:47,250 --> 00:16:49,920
new binary right to production you don't

347
00:16:49,920 --> 00:16:51,210
want to push a new model right to

348
00:16:51,210 --> 00:16:53,160
production either so having these

349
00:16:53,160 --> 00:16:56,340
multiple models in memory lets you be

350
00:16:56,340 --> 00:16:58,620
serving one model on production traffic

351
00:16:58,620 --> 00:17:00,780
and load a new one and maybe send it

352
00:17:00,780 --> 00:17:02,910
some canary request send it some QA

353
00:17:02,910 --> 00:17:04,800
requests make sure everything's all

354
00:17:04,800 --> 00:17:05,040
right

355
00:17:05,040 --> 00:17:07,589
and then move the traffic over to that

356
00:17:07,589 --> 00:17:11,819
new model and this supports doing things

357
00:17:11,819 --> 00:17:14,610
like reloading if you have a stream of

358
00:17:14,610 --> 00:17:16,199
models you're producing pensive low

359
00:17:16,199 --> 00:17:18,689
serving will transparently load the new

360
00:17:18,689 --> 00:17:22,350
ones and unload the old ones we've built

361
00:17:22,350 --> 00:17:24,839
in a lot of isolation if you have a

362
00:17:24,839 --> 00:17:26,699
model that's serving a lot of traffic in

363
00:17:26,699 --> 00:17:28,950
one thread and it's time to load a new

364
00:17:28,950 --> 00:17:30,160
model you make sure

365
00:17:30,160 --> 00:17:31,780
do that in a separate thread that way we

366
00:17:31,780 --> 00:17:33,790
don't cause any hiccups in the thread

367
00:17:33,790 --> 00:17:37,050
that's serving production traffic and

368
00:17:37,050 --> 00:17:40,300
again this entire system has been built

369
00:17:40,300 --> 00:17:42,040
from the ground up to be very high

370
00:17:42,040 --> 00:17:44,080
throughput things like selecting those

371
00:17:44,080 --> 00:17:46,420
different models based on the name or

372
00:17:46,420 --> 00:17:48,490
selecting different versions that's very

373
00:17:48,490 --> 00:17:51,490
very efficient similarly it has some

374
00:17:51,490 --> 00:17:54,610
advanced batching right this way we can

375
00:17:54,610 --> 00:17:56,890
make use of accelerators we also see

376
00:17:56,890 --> 00:18:00,280
improvements on standard CPUs with this

377
00:18:00,280 --> 00:18:02,530
batching and then lots of other

378
00:18:02,530 --> 00:18:04,480
enhancements everything from protocol

379
00:18:04,480 --> 00:18:09,370
buffer magic to lots more and this is

380
00:18:09,370 --> 00:18:11,650
really what we use inside Google to

381
00:18:11,650 --> 00:18:14,190
serve tensorflow I think there's over

382
00:18:14,190 --> 00:18:17,560
1,500 projects that use it it serves

383
00:18:17,560 --> 00:18:18,970
somewhere in the neighborhood of 10

384
00:18:18,970 --> 00:18:22,000
million QPS which ends up being about a

385
00:18:22,000 --> 00:18:24,970
hundred million items predicted per

386
00:18:24,970 --> 00:18:27,610
second and we're also seeing some

387
00:18:27,610 --> 00:18:32,380
adoption outside of Google one of the

388
00:18:32,380 --> 00:18:33,730
new things I'd like to share today is

389
00:18:33,730 --> 00:18:37,750
distributed serving so looking inside

390
00:18:37,750 --> 00:18:39,640
Google we've seen a couple of trends one

391
00:18:39,640 --> 00:18:41,230
is that models are getting bigger and

392
00:18:41,230 --> 00:18:43,270
bigger some of the ones inside Google

393
00:18:43,270 --> 00:18:47,500
are over a terabyte in size the other

394
00:18:47,500 --> 00:18:49,480
thing we're seeing is this sharing of

395
00:18:49,480 --> 00:18:51,790
sub-graphs right TF hub is producing

396
00:18:51,790 --> 00:18:55,000
these common pieces of models and we're

397
00:18:55,000 --> 00:18:56,500
also seeing more and more specialization

398
00:18:56,500 --> 00:18:59,050
in these models as they get bigger and

399
00:18:59,050 --> 00:19:01,270
bigger right if you look at some of

400
00:19:01,270 --> 00:19:03,190
these model structures they look less

401
00:19:03,190 --> 00:19:04,960
like a model that would belong on one

402
00:19:04,960 --> 00:19:07,090
machine and more like an entire system

403
00:19:07,090 --> 00:19:09,670
so that's this is exactly what

404
00:19:09,670 --> 00:19:11,740
distributed serving is meant for kinda

405
00:19:11,740 --> 00:19:13,830
lets us take the single model and

406
00:19:13,830 --> 00:19:16,000
basically break it up into micro

407
00:19:16,000 --> 00:19:17,620
services so to get a better feel for

408
00:19:17,620 --> 00:19:21,160
that will say that andrew has taken his

409
00:19:21,160 --> 00:19:23,350
rabbit classifier and is serving it on a

410
00:19:23,350 --> 00:19:26,440
model server and we'll say that I want

411
00:19:26,440 --> 00:19:29,860
to create a similar system to classify

412
00:19:29,860 --> 00:19:32,080
cat breeds and so I've done the same

413
00:19:32,080 --> 00:19:33,880
thing I've started from tensorflow hub

414
00:19:33,880 --> 00:19:36,310
so you can see I've got the tensorflow

415
00:19:36,310 --> 00:19:39,030
hub module in the center there and

416
00:19:39,030 --> 00:19:41,500
you'll notice that since we both started

417
00:19:41,500 --> 00:19:43,149
from the same module we

418
00:19:43,149 --> 00:19:45,429
the same bits of code we have the same

419
00:19:45,429 --> 00:19:48,879
core to our mission their model so what

420
00:19:48,879 --> 00:19:51,279
we can do is we can start a third server

421
00:19:51,279 --> 00:19:53,769
and we can put the tensorflow hub module

422
00:19:53,769 --> 00:19:56,950
on that server and we can remove it from

423
00:19:56,950 --> 00:19:59,109
the servers on the outside and leave in

424
00:19:59,109 --> 00:20:00,759
its place this placeholder we call a

425
00:20:00,759 --> 00:20:02,830
remote op you can think of this as a

426
00:20:02,830 --> 00:20:05,919
portal it's kind of a forwarding op that

427
00:20:05,919 --> 00:20:08,649
when we run the inference it forwards at

428
00:20:08,649 --> 00:20:10,359
the appropriate point in the in the

429
00:20:10,359 --> 00:20:14,139
processing to the model server there the

430
00:20:14,139 --> 00:20:16,089
computation is done and the result gets

431
00:20:16,089 --> 00:20:18,639
sent back and the computation continues

432
00:20:18,639 --> 00:20:22,239
on our classifiers on the outside so

433
00:20:22,239 --> 00:20:23,469
there's a few reasons we might want to

434
00:20:23,469 --> 00:20:24,999
do this right we can get rid of some

435
00:20:24,999 --> 00:20:28,089
duplication now we only have one model

436
00:20:28,089 --> 00:20:31,509
server loading all these weights we also

437
00:20:31,509 --> 00:20:33,719
get the benefit that that can batch

438
00:20:33,719 --> 00:20:35,799
requests that are coming from both sides

439
00:20:35,799 --> 00:20:38,049
and also we can set up different

440
00:20:38,049 --> 00:20:39,639
configurations you can imagine we might

441
00:20:39,639 --> 00:20:41,919
have this model server just loaded with

442
00:20:41,919 --> 00:20:44,679
TP use our tensor processing units so

443
00:20:44,679 --> 00:20:46,919
that it can do what are most likely

444
00:20:46,919 --> 00:20:48,969
convolutional operations and things like

445
00:20:48,969 --> 00:20:52,869
that very efficiently so another place

446
00:20:52,869 --> 00:20:55,899
where we use this is with large sharded

447
00:20:55,899 --> 00:20:58,059
models so if you're familiar with deep

448
00:20:58,059 --> 00:21:00,070
learning there's this technique of

449
00:21:00,070 --> 00:21:03,219
embedding things like words or YouTube

450
00:21:03,219 --> 00:21:09,219
video IDs as a string of numbers right

451
00:21:09,219 --> 00:21:11,739
we represent them as this vector of

452
00:21:11,739 --> 00:21:13,479
numbers and if you have a lot of words

453
00:21:13,479 --> 00:21:15,149
or you have a lot of YouTube videos

454
00:21:15,149 --> 00:21:18,369
you're gonna have a lot of data so much

455
00:21:18,369 --> 00:21:20,769
that it won't fit on one machine so we

456
00:21:20,769 --> 00:21:23,409
use a system like this to split up those

457
00:21:23,409 --> 00:21:26,200
embeddings for the words into these

458
00:21:26,200 --> 00:21:29,049
shards and we can distribute there and

459
00:21:29,049 --> 00:21:30,460
of course the main model when it needs

460
00:21:30,460 --> 00:21:32,559
something can reach out get it and then

461
00:21:32,559 --> 00:21:37,809
do the computation another example is

462
00:21:37,809 --> 00:21:40,059
what we call triggering models so we'll

463
00:21:40,059 --> 00:21:42,159
say we're building a spam detector and

464
00:21:42,159 --> 00:21:44,200
we have a full model which is a very

465
00:21:44,200 --> 00:21:46,450
very powerful spam detector you know

466
00:21:46,450 --> 00:21:48,129
maybe it looks at the words understands

467
00:21:48,129 --> 00:21:50,349
the context it's very powerful but it's

468
00:21:50,349 --> 00:21:52,450
very expensive and we can't afford to

469
00:21:52,450 --> 00:21:55,509
run it on every single email message we

470
00:21:55,509 --> 00:21:55,990
get

471
00:21:55,990 --> 00:21:57,880
so what we do instead is we put this

472
00:21:57,880 --> 00:22:00,160
triggering model in front of it as you

473
00:22:00,160 --> 00:22:02,260
can imagine there's a lot of cases where

474
00:22:02,260 --> 00:22:04,630
we're in a position to very quickly say

475
00:22:04,630 --> 00:22:07,210
yes this is spam or no it's not so for

476
00:22:07,210 --> 00:22:08,620
instance if we get an email that's from

477
00:22:08,620 --> 00:22:11,530
within our own domain maybe we can just

478
00:22:11,530 --> 00:22:13,600
say that's not spam and the triggering

479
00:22:13,600 --> 00:22:15,910
model can quickly return that if it's

480
00:22:15,910 --> 00:22:17,440
something that's difficult it can go

481
00:22:17,440 --> 00:22:19,600
ahead and forward that on to the full

482
00:22:19,600 --> 00:22:27,160
model where it will process it so a

483
00:22:27,160 --> 00:22:29,110
similar concept is this mixture of

484
00:22:29,110 --> 00:22:31,510
experts so in this case let's say we

485
00:22:31,510 --> 00:22:33,280
want to build a system where we're going

486
00:22:33,280 --> 00:22:36,850
to classify the breed of either a rabbit

487
00:22:36,850 --> 00:22:40,240
or a cat so what we're gonna do is we're

488
00:22:40,240 --> 00:22:41,830
gonna have two models we're gonna call

489
00:22:41,830 --> 00:22:44,260
expert models right so we have one

490
00:22:44,260 --> 00:22:47,320
that's an expert at rabbits and another

491
00:22:47,320 --> 00:22:51,610
that's an expert at cats and so here

492
00:22:51,610 --> 00:22:53,830
we're gonna use a gating model to get a

493
00:22:53,830 --> 00:22:55,990
picture of either a rabbit or cat and

494
00:22:55,990 --> 00:22:57,730
the only thing that's gonna do is decide

495
00:22:57,730 --> 00:23:00,070
if it's a rabbit or a cat and forward it

496
00:23:00,070 --> 00:23:04,180
on to the appropriate expert who will

497
00:23:04,180 --> 00:23:05,970
process it and we'll send that that

498
00:23:05,970 --> 00:23:11,200
result back all right there's lots of

499
00:23:11,200 --> 00:23:12,670
use cases we're excited to see what

500
00:23:12,670 --> 00:23:15,400
people start to build with these remote

501
00:23:15,400 --> 00:23:17,830
ops the next thing I'll quickly mention

502
00:23:17,830 --> 00:23:21,220
is a REST API this was one of the top

503
00:23:21,220 --> 00:23:23,620
requests on github so we're happy to be

504
00:23:23,620 --> 00:23:25,420
releasing this soon this will make it

505
00:23:25,420 --> 00:23:27,190
much easier to integrate things with

506
00:23:27,190 --> 00:23:31,210
existing existing services and it's nice

507
00:23:31,210 --> 00:23:32,350
because you don't actually have to

508
00:23:32,350 --> 00:23:34,420
choose on one model server with one

509
00:23:34,420 --> 00:23:37,410
tensorflow model you can serve either

510
00:23:37,410 --> 00:23:42,150
the restful endpoint or the GRP see

511
00:23:42,150 --> 00:23:44,920
there's three api's there's some

512
00:23:44,920 --> 00:23:46,210
higher-level ones like for

513
00:23:46,210 --> 00:23:48,640
classification and regression there's

514
00:23:48,640 --> 00:23:49,750
also a lower-level

515
00:23:49,750 --> 00:23:51,940
predict and this is more of a tensor in

516
00:23:51,940 --> 00:23:54,280
tensor out for the things that don't fit

517
00:23:54,280 --> 00:23:58,240
into classify and regress so looking at

518
00:23:58,240 --> 00:24:01,570
this quickly you can see the URI here we

519
00:24:01,570 --> 00:24:04,240
can specify the model right this may be

520
00:24:04,240 --> 00:24:05,950
like rabbit or cat

521
00:24:05,950 --> 00:24:08,110
we can optionally specify a version and

522
00:24:08,110 --> 00:24:10,870
our verbs are the classify regress and

523
00:24:10,870 --> 00:24:13,510
predict we have two examples the first

524
00:24:13,510 --> 00:24:16,690
one you can see we're asking the iris

525
00:24:16,690 --> 00:24:20,050
model to classify something in here we

526
00:24:20,050 --> 00:24:22,240
aren't giving it a version a model

527
00:24:22,240 --> 00:24:23,530
version so it'll just use the most

528
00:24:23,530 --> 00:24:25,000
recent or the highest version

529
00:24:25,000 --> 00:24:27,730
automatically and the bottom example is

530
00:24:27,730 --> 00:24:29,770
one where we're using the M NIST model

531
00:24:29,770 --> 00:24:32,620
and we're specifying the version to be 3

532
00:24:32,620 --> 00:24:36,220
1 4 and asking it to do a prediction so

533
00:24:36,220 --> 00:24:38,710
this lets you this lets you easily

534
00:24:38,710 --> 00:24:42,040
integrate things and easily version

535
00:24:42,040 --> 00:24:44,980
models and switch between them I'll

536
00:24:44,980 --> 00:24:46,360
quickly mention the API if you're

537
00:24:46,360 --> 00:24:48,670
familiar with tensorflow example you

538
00:24:48,670 --> 00:24:50,740
know that representing it in JSON is a

539
00:24:50,740 --> 00:24:52,270
little bit cumbersome so you can see

540
00:24:52,270 --> 00:24:54,190
it's pretty verbose here there's some

541
00:24:54,190 --> 00:24:56,080
other warts like needing to encode

542
00:24:56,080 --> 00:25:00,040
things base64 instead with tensorflow

543
00:25:00,040 --> 00:25:02,380
serving the RESTful API uses a more

544
00:25:02,380 --> 00:25:04,330
idiomatic JSON which is much more

545
00:25:04,330 --> 00:25:09,070
pleasant much more succinct and here

546
00:25:09,070 --> 00:25:11,020
this last example just kind of pulls it

547
00:25:11,020 --> 00:25:12,700
all together where you can use curl to

548
00:25:12,700 --> 00:25:15,280
actually make predictions from the

549
00:25:15,280 --> 00:25:18,850
command-line so I encourage you to check

550
00:25:18,850 --> 00:25:20,470
out the project that tensorflow serving

551
00:25:20,470 --> 00:25:22,210
there's lots of great documentation and

552
00:25:22,210 --> 00:25:24,280
things like that and we also welcome

553
00:25:24,280 --> 00:25:27,580
contributions and code discussion ideas

554
00:25:27,580 --> 00:25:31,720
on our github project page so I'd like

555
00:25:31,720 --> 00:25:33,610
to finish with James to talk about

556
00:25:33,610 --> 00:25:39,600
tensorflow extended

557
00:25:39,600 --> 00:25:45,370
like all right so I'm gonna start with a

558
00:25:45,370 --> 00:25:49,030
single non-controversial statement this

559
00:25:49,030 --> 00:25:51,730
has been shown true made many times by

560
00:25:51,730 --> 00:25:55,000
many people in short tf-x

561
00:25:55,000 --> 00:25:59,020
is our answer to that statement I'll

562
00:25:59,020 --> 00:26:01,750
start with a simple diagram this core

563
00:26:01,750 --> 00:26:03,309
box represents your machine learning

564
00:26:03,309 --> 00:26:03,760
code

565
00:26:03,760 --> 00:26:05,890
this is the magic bits of algorithms

566
00:26:05,890 --> 00:26:07,900
that actually take the data in and

567
00:26:07,900 --> 00:26:11,830
produce reasonable results the blue

568
00:26:11,830 --> 00:26:14,020
boxes represent everything else you need

569
00:26:14,020 --> 00:26:15,450
to actually use machine learning

570
00:26:15,450 --> 00:26:18,670
reliably and scalably in an actual real

571
00:26:18,670 --> 00:26:21,280
production setting the blue boxes are

572
00:26:21,280 --> 00:26:22,420
going to be where you're spending most

573
00:26:22,420 --> 00:26:24,250
of your time it comprises most of the

574
00:26:24,250 --> 00:26:26,170
lines of code it's also going to be the

575
00:26:26,170 --> 00:26:27,490
source of most of the things that are

576
00:26:27,490 --> 00:26:28,840
selling off your pagers in the middle of

577
00:26:28,840 --> 00:26:32,260
the night in our case if we squint at

578
00:26:32,260 --> 00:26:34,900
this just about correctly the core ml

579
00:26:34,900 --> 00:26:37,270
box looks like tensorflow and all of the

580
00:26:37,270 --> 00:26:39,600
blue boxes together comprise tf-x

581
00:26:39,600 --> 00:26:42,160
so we're gonna quickly run through four

582
00:26:42,160 --> 00:26:44,230
of the key principles that tf-x was

583
00:26:44,230 --> 00:26:48,070
built on first Express ability and tf-x

584
00:26:48,070 --> 00:26:50,620
is gonna be flexible in three ways first

585
00:26:50,620 --> 00:26:53,230
of all we're gonna take advantage of the

586
00:26:53,230 --> 00:26:55,210
flexibility built into tensorflow using

587
00:26:55,210 --> 00:26:57,700
it as our trainer means that we can do

588
00:26:57,700 --> 00:26:58,960
anything tensor for look into at the

589
00:26:58,960 --> 00:27:00,670
model level which means you can have

590
00:27:00,670 --> 00:27:01,450
wide models

591
00:27:01,450 --> 00:27:03,580
deep models supervised models

592
00:27:03,580 --> 00:27:06,850
unsupervised tree models anything that

593
00:27:06,850 --> 00:27:10,420
we can whip up together second were

594
00:27:10,420 --> 00:27:13,240
flexible with regards to input data we

595
00:27:13,240 --> 00:27:15,600
could handle images texts sparse data

596
00:27:15,600 --> 00:27:17,530
multimodal models where you might want

597
00:27:17,530 --> 00:27:20,440
to Train images and surrounding text or

598
00:27:20,440 --> 00:27:24,390
something like videos plus captions

599
00:27:24,390 --> 00:27:26,410
third there are multiple ways you might

600
00:27:26,410 --> 00:27:28,750
go about actually training a model if

601
00:27:28,750 --> 00:27:30,309
your goal is to build a kitten detector

602
00:27:30,309 --> 00:27:32,230
you may have all of your data up front

603
00:27:32,230 --> 00:27:34,179
and your goal may be to build one model

604
00:27:34,179 --> 00:27:35,530
of sufficient high quality and then

605
00:27:35,530 --> 00:27:36,600
you're done

606
00:27:36,600 --> 00:27:39,220
in contrast to that if your goal is to

607
00:27:39,220 --> 00:27:42,090
build a viral kitten video detector or a

608
00:27:42,090 --> 00:27:45,340
personalized kitten recommender then

609
00:27:45,340 --> 00:27:46,480
you're not going to have all of your

610
00:27:46,480 --> 00:27:48,580
data up front so typically your trade a

611
00:27:48,580 --> 00:27:50,710
model get it into production and then as

612
00:27:50,710 --> 00:27:52,080
data comes in

613
00:27:52,080 --> 00:27:53,460
you'll throw away that model and train a

614
00:27:53,460 --> 00:27:55,590
new model and then throw away that model

615
00:27:55,590 --> 00:27:57,870
and train a new model we're actually

616
00:27:57,870 --> 00:27:59,429
throwing out some good data along with

617
00:27:59,429 --> 00:28:02,220
these models though so we can try a worm

618
00:28:02,220 --> 00:28:04,559
starting strategy instead where we'll

619
00:28:04,559 --> 00:28:07,140
continuously train the same model but as

620
00:28:07,140 --> 00:28:09,659
data comes in we'll warm start based on

621
00:28:09,659 --> 00:28:11,429
the previous state of the model and just

622
00:28:11,429 --> 00:28:14,519
add the additional new data this will

623
00:28:14,519 --> 00:28:16,860
let us result in higher quality models

624
00:28:16,860 --> 00:28:22,500
with faster convergence next let's talk

625
00:28:22,500 --> 00:28:24,779
about portability so each of the tf-x

626
00:28:24,779 --> 00:28:26,760
modules represented by the blue boxes

627
00:28:26,760 --> 00:28:28,649
don't need to do all of the heavy

628
00:28:28,649 --> 00:28:30,419
lifting themselves they're part of an

629
00:28:30,419 --> 00:28:32,970
open source ecosystem which means we can

630
00:28:32,970 --> 00:28:34,740
lean on things like tensorflow and take

631
00:28:34,740 --> 00:28:38,070
advantage of its native portability this

632
00:28:38,070 --> 00:28:41,220
means we can run locally we can scale up

633
00:28:41,220 --> 00:28:43,860
and run in a cloud environment we can

634
00:28:43,860 --> 00:28:45,149
scale to devices that you're thinking

635
00:28:45,149 --> 00:28:48,570
about today and to devices that you

636
00:28:48,570 --> 00:28:53,580
might be thinking about tomorrow a large

637
00:28:53,580 --> 00:28:55,139
portion of machine learning is data

638
00:28:55,139 --> 00:28:57,600
processing so we rely on Apache beam

639
00:28:57,600 --> 00:29:00,330
which is built for this task and again

640
00:29:00,330 --> 00:29:02,130
we can take advantage of beams

641
00:29:02,130 --> 00:29:04,710
portability as our own which means we

642
00:29:04,710 --> 00:29:06,299
can use the direct runner locally where

643
00:29:06,299 --> 00:29:07,470
you might be starting out with a small

644
00:29:07,470 --> 00:29:09,919
piece of data building small models to

645
00:29:09,919 --> 00:29:12,120
affirm that your approaches are actually

646
00:29:12,120 --> 00:29:15,210
correct and then scale up into the cloud

647
00:29:15,210 --> 00:29:17,909
with a data flow rudder also utilize

648
00:29:17,909 --> 00:29:19,610
something like the flink runner or

649
00:29:19,610 --> 00:29:21,750
things that are in progress right now

650
00:29:21,750 --> 00:29:26,429
like a spark Runner will see the same

651
00:29:26,429 --> 00:29:28,500
story again with kubernetes where we can

652
00:29:28,500 --> 00:29:30,559
start with mini cube running locally

653
00:29:30,559 --> 00:29:33,570
scale up into the cloud or two clusters

654
00:29:33,570 --> 00:29:35,990
that we have for other purposes and

655
00:29:35,990 --> 00:29:38,399
eventually scale to things that don't

656
00:29:38,399 --> 00:29:43,130
yet exist but they're still in progress

657
00:29:43,130 --> 00:29:45,450
so portability is only part of the

658
00:29:45,450 --> 00:29:48,600
scalability story traditionally we've

659
00:29:48,600 --> 00:29:50,880
seen two very different roles involved

660
00:29:50,880 --> 00:29:51,960
in machine learnings you'll have the

661
00:29:51,960 --> 00:29:54,149
data scientists on one side and be

662
00:29:54,149 --> 00:29:56,309
production infrastructure engineers on

663
00:29:56,309 --> 00:29:59,700
the other side the differences between

664
00:29:59,700 --> 00:30:02,039
these are not just amounts of data but

665
00:30:02,039 --> 00:30:04,169
there are key concerns that each has

666
00:30:04,169 --> 00:30:05,940
about as they go about their daily busy

667
00:30:05,940 --> 00:30:08,340
this with tf-x we can specifically

668
00:30:08,340 --> 00:30:10,350
target use cases that are in common

669
00:30:10,350 --> 00:30:12,029
between the two as well as things that

670
00:30:12,029 --> 00:30:14,399
are specific to the two so this will

671
00:30:14,399 --> 00:30:16,200
allow us to have one unified system that

672
00:30:16,200 --> 00:30:19,529
can scale up to the cloud and down to

673
00:30:19,529 --> 00:30:22,139
smaller environments and actually unlock

674
00:30:22,139 --> 00:30:26,509
collaboration between these two roles

675
00:30:26,509 --> 00:30:28,500
finally we believe heavily in

676
00:30:28,500 --> 00:30:30,809
interactivity we were able to get quick

677
00:30:30,809 --> 00:30:33,269
iterative results with responsive

678
00:30:33,269 --> 00:30:35,429
tooling and fast debugging and there's

679
00:30:35,429 --> 00:30:38,519
interactivity should remain such even at

680
00:30:38,519 --> 00:30:42,210
scale with large sets of data or large

681
00:30:42,210 --> 00:30:47,159
models this is a fairly ambitious goal

682
00:30:47,159 --> 00:30:51,299
so where are we now so today we've

683
00:30:51,299 --> 00:30:53,519
open-sourced a few key areas of

684
00:30:53,519 --> 00:30:55,559
responsibility so we have transform

685
00:30:55,559 --> 00:30:59,190
model analysis serving and facets each

686
00:30:59,190 --> 00:31:01,649
one of these is useful on its own but is

687
00:31:01,649 --> 00:31:03,809
much more so when used in concert with

688
00:31:03,809 --> 00:31:06,720
the others so let's walk through what

689
00:31:06,720 --> 00:31:11,159
this might look like in practice so our

690
00:31:11,159 --> 00:31:12,659
goal here is to take a bunch of data

691
00:31:12,659 --> 00:31:14,700
we've accumulated and do something

692
00:31:14,700 --> 00:31:17,299
useful for our users of our product

693
00:31:17,299 --> 00:31:19,259
these are the steps we want to take

694
00:31:19,259 --> 00:31:21,360
along the way so let's start with step

695
00:31:21,360 --> 00:31:24,870
one with the data we're going to pull

696
00:31:24,870 --> 00:31:27,120
this up in facets and use it to actually

697
00:31:27,120 --> 00:31:29,879
analyze what features might be useful

698
00:31:29,879 --> 00:31:33,299
predictors look for any anomalies so

699
00:31:33,299 --> 00:31:35,399
outliers in our data or missing features

700
00:31:35,399 --> 00:31:37,649
to try to avoid the classic garbage in

701
00:31:37,649 --> 00:31:40,529
garbage out problem and to try to inform

702
00:31:40,529 --> 00:31:42,299
what data we're going to need to further

703
00:31:42,299 --> 00:31:44,429
pre-process before it's useful for our

704
00:31:44,429 --> 00:31:48,269
ml training which leads into our next

705
00:31:48,269 --> 00:31:50,820
step which is to actually use transform

706
00:31:50,820 --> 00:31:53,309
to transform our features so TF

707
00:31:53,309 --> 00:31:55,710
transform will let you do full pass

708
00:31:55,710 --> 00:31:57,690
analysis and transforms of your base

709
00:31:57,690 --> 00:32:00,720
data and it's also very firmly attached

710
00:32:00,720 --> 00:32:03,539
to the TF graph itself which will ensure

711
00:32:03,539 --> 00:32:05,759
that you're applying the same transforms

712
00:32:05,759 --> 00:32:10,799
in training as in serving from the code

713
00:32:10,799 --> 00:32:12,029
you can see that we're taking advantage

714
00:32:12,029 --> 00:32:14,789
of a few ops built into transform and we

715
00:32:14,789 --> 00:32:16,740
could do things like scale generate

716
00:32:16,740 --> 00:32:18,539
vocabularies or bucket eyes our base

717
00:32:18,539 --> 00:32:19,440
data

718
00:32:19,440 --> 00:32:20,730
and this code will look the same

719
00:32:20,730 --> 00:32:23,309
regardless of our execution environment

720
00:32:23,309 --> 00:32:25,019
and of course if you needed to find your

721
00:32:25,019 --> 00:32:29,279
own operations you can do so so this

722
00:32:29,279 --> 00:32:30,809
puts us at the point where we're

723
00:32:30,809 --> 00:32:33,059
strongly suspicious that we have data we

724
00:32:33,059 --> 00:32:35,519
can actually use to generate a model so

725
00:32:35,519 --> 00:32:38,549
let's look at doing that we're going to

726
00:32:38,549 --> 00:32:40,259
use a ten circle estimator which is a

727
00:32:40,259 --> 00:32:45,409
high level API that will let us quickly

728
00:32:45,409 --> 00:32:49,500
define train and export our model this

729
00:32:49,500 --> 00:32:53,129
is a small set of estimators that are

730
00:32:53,129 --> 00:32:54,629
present in court tensorflow there are a

731
00:32:54,629 --> 00:32:56,039
lot more available and you can also

732
00:32:56,039 --> 00:33:00,870
create your own we're going to look

733
00:33:00,870 --> 00:33:02,700
ahead to some future steps and we're

734
00:33:02,700 --> 00:33:04,980
gonna purposefully export two graphs

735
00:33:04,980 --> 00:33:07,350
into our saved model one specific

736
00:33:07,350 --> 00:33:07,860
disserving

737
00:33:07,860 --> 00:33:12,169
and one specific to model evaluation and

738
00:33:12,169 --> 00:33:14,340
again from the code you can see that

739
00:33:14,340 --> 00:33:15,360
we're good in this case we're going to

740
00:33:15,360 --> 00:33:17,639
use a white and deep model we're gonna

741
00:33:17,639 --> 00:33:19,379
define it we're gonna train it we're

742
00:33:19,379 --> 00:33:23,850
gonna do our exports so now we have a

743
00:33:23,850 --> 00:33:26,309
model we could just push this directly

744
00:33:26,309 --> 00:33:27,600
to production but that would probably be

745
00:33:27,600 --> 00:33:29,549
a very bad idea so let's try to gain a

746
00:33:29,549 --> 00:33:31,019
little more confidence and what would

747
00:33:31,019 --> 00:33:33,240
happen if we actually did so for our

748
00:33:33,240 --> 00:33:37,259
end-users so we're gonna step into TF

749
00:33:37,259 --> 00:33:39,870
model analysis we're gonna utilize this

750
00:33:39,870 --> 00:33:42,720
to evaluate our model over a large data

751
00:33:42,720 --> 00:33:46,200
set and then we're going to define in

752
00:33:46,200 --> 00:33:48,480
this case one but you could possibly use

753
00:33:48,480 --> 00:33:51,570
many slices of this data that we want to

754
00:33:51,570 --> 00:33:56,730
analyze independently from others this

755
00:33:56,730 --> 00:33:58,350
will allow us to actually look at

756
00:33:58,350 --> 00:34:00,090
subsets of our data that may be

757
00:34:00,090 --> 00:34:01,860
representative of subsets of our users

758
00:34:01,860 --> 00:34:04,830
and how our metrics actually track

759
00:34:04,830 --> 00:34:08,639
between these groups for example you may

760
00:34:08,639 --> 00:34:10,740
have sets of users in different

761
00:34:10,740 --> 00:34:12,929
languages maybe access different devices

762
00:34:12,929 --> 00:34:15,869
or maybe you have a very small but

763
00:34:15,869 --> 00:34:17,429
passionate community of rabbit

764
00:34:17,429 --> 00:34:19,260
aficionados mixed in with your larger

765
00:34:19,260 --> 00:34:22,050
community of kitten fanatics and you

766
00:34:22,050 --> 00:34:23,280
want to make sure that your model will

767
00:34:23,280 --> 00:34:26,669
actually give a positive experiences to

768
00:34:26,669 --> 00:34:30,440
both groups equally

769
00:34:30,440 --> 00:34:32,490
so now we have a model that we're

770
00:34:32,490 --> 00:34:34,350
confident in and we want to push it to

771
00:34:34,350 --> 00:34:36,690
serving so let's get this up and for

772
00:34:36,690 --> 00:34:41,430
some queries at it so this is quick now

773
00:34:41,430 --> 00:34:43,410
we have a model up we have a server

774
00:34:43,410 --> 00:34:45,420
listening on port 9000 for G RPC

775
00:34:45,420 --> 00:34:47,250
requests so now we're going to back out

776
00:34:47,250 --> 00:34:50,280
into our actual products code we can

777
00:34:50,280 --> 00:34:52,050
assemble individual prediction requests

778
00:34:52,050 --> 00:34:53,190
and then we can send them out to our

779
00:34:53,190 --> 00:34:54,170
server

780
00:34:54,170 --> 00:34:57,570
and if this slide doesn't look like your

781
00:34:57,570 --> 00:34:59,910
actual code and this one looks more

782
00:34:59,910 --> 00:35:01,680
similar than you'll be happy to see that

783
00:35:01,680 --> 00:35:03,480
this is coming soon I'm treating a

784
00:35:03,480 --> 00:35:05,010
little by showing you this now as

785
00:35:05,010 --> 00:35:06,750
current state but we're super excited

786
00:35:06,750 --> 00:35:08,640
about this and this is one of those real

787
00:35:08,640 --> 00:35:13,610
soon now scenarios so that's today

788
00:35:13,610 --> 00:35:19,340
what's coming next so first please

789
00:35:19,340 --> 00:35:21,750
contribute and join the tensor flood org

790
00:35:21,750 --> 00:35:23,340
community we don't want the only time

791
00:35:23,340 --> 00:35:24,840
that we're talking back and forth here

792
00:35:24,840 --> 00:35:28,610
to be at summits and conferences

793
00:35:28,610 --> 00:35:31,560
secondly some of you may have seen the

794
00:35:31,560 --> 00:35:34,470
tf-x paper at kdd last year this

795
00:35:34,470 --> 00:35:36,450
specifies what we believe an end-to-end

796
00:35:36,450 --> 00:35:40,530
platform actually looks like here it is

797
00:35:40,530 --> 00:35:44,460
and by we believing that this is what it

798
00:35:44,460 --> 00:35:46,380
looks like this is what it looks like

799
00:35:46,380 --> 00:35:48,270
this is actually what's powering some of

800
00:35:48,270 --> 00:35:50,430
the pretty awesome AI first products

801
00:35:50,430 --> 00:35:51,750
that you've been seeing at i/o and that

802
00:35:51,750 --> 00:35:54,980
you've probably been using yourselves

803
00:35:54,980 --> 00:35:57,570
but again this is where we are now ss

804
00:35:57,570 --> 00:36:00,120
right now this is not the full platform

805
00:36:00,120 --> 00:36:01,830
but you can see what we're aiming for

806
00:36:01,830 --> 00:36:07,800
and we'll get there eventually so again

807
00:36:07,800 --> 00:36:11,760
please download this software use it to

808
00:36:11,760 --> 00:36:14,220
make good things and send us feedback

809
00:36:14,220 --> 00:36:18,900
and thank you from all of us for being

810
00:36:18,900 --> 00:36:21,000
current and future users and for

811
00:36:21,000 --> 00:36:22,200
choosing to spend your time with us

812
00:36:22,200 --> 00:36:22,480
today

813
00:36:22,480 --> 00:36:25,840
[Applause]

814
00:36:25,840 --> 00:36:48,349
[Music]

