1
00:00:03,350 --> 00:00:05,609
hi everybody and welcome to tents Flo

2
00:00:05,609 --> 00:00:07,529
meets in this episode I'm meeting with

3
00:00:07,529 --> 00:00:09,240
Christian fan Li Ramsey who's been doing

4
00:00:09,240 --> 00:00:11,610
some crazy stuff with neuroscience and a

5
00:00:11,610 --> 00:00:13,110
whole new term called affective

6
00:00:13,110 --> 00:00:14,370
computing that you're going to learn all

7
00:00:14,370 --> 00:00:16,859
about welcome Christian thanks I'm glad

8
00:00:16,859 --> 00:00:18,570
to be here so we did have another

9
00:00:18,570 --> 00:00:20,310
episode with your partner how and where

10
00:00:20,310 --> 00:00:21,960
she was talking a lot about diet ex

11
00:00:21,960 --> 00:00:23,460
machina and I know you're working on

12
00:00:23,460 --> 00:00:24,930
that too could you tell us about it sure

13
00:00:24,930 --> 00:00:29,130
sure so diet ex machina is again the

14
00:00:29,130 --> 00:00:31,590
diet part is about two as one so it's

15
00:00:31,590 --> 00:00:33,690
seen two is one and then the machinae is

16
00:00:33,690 --> 00:00:36,000
about really us working alongside a

17
00:00:36,000 --> 00:00:37,620
machine so it's really a fancy way to

18
00:00:37,620 --> 00:00:39,780
say to people working with a machine

19
00:00:39,780 --> 00:00:42,239
nice and so what we're trying to build

20
00:00:42,239 --> 00:00:45,090
out is this effective layer and to us it

21
00:00:45,090 --> 00:00:47,610
means that well currently as it stands

22
00:00:47,610 --> 00:00:48,809
if you look at the neuroscience

23
00:00:48,809 --> 00:00:51,480
literature emotion is just as important

24
00:00:51,480 --> 00:00:53,219
it's cognition okay but in today's

25
00:00:53,219 --> 00:00:55,680
society we seem to value cognition over

26
00:00:55,680 --> 00:00:57,570
emotion then we're kind of like throw

27
00:00:57,570 --> 00:00:59,309
their emotions away get them out of here

28
00:00:59,309 --> 00:01:01,949
Larry : yes make a reasonable decision

29
00:01:01,949 --> 00:01:03,930
but it turns out that to make a

30
00:01:03,930 --> 00:01:05,549
reasonable decision you actually have to

31
00:01:05,549 --> 00:01:07,560
have a marker write an emotional marker

32
00:01:07,560 --> 00:01:08,760
that tells you that's the right answer

33
00:01:08,760 --> 00:01:10,380
and that one's not the right answer

34
00:01:10,380 --> 00:01:12,600
interesting and so we're kind of trying

35
00:01:12,600 --> 00:01:14,700
to build this affective layer to bring

36
00:01:14,700 --> 00:01:17,070
emotion back to the forefront of what

37
00:01:17,070 --> 00:01:18,930
what the everyday life is or really like

38
00:01:18,930 --> 00:01:20,580
okay so I can give you an example of

39
00:01:20,580 --> 00:01:22,710
that okay let's imagine we're working on

40
00:01:22,710 --> 00:01:25,080
the team and you've got all of a sudden

41
00:01:25,080 --> 00:01:27,119
this great idea that you want to tell

42
00:01:27,119 --> 00:01:29,400
everyone about and what's going to

43
00:01:29,400 --> 00:01:31,200
happen is most likely you're gonna have

44
00:01:31,200 --> 00:01:33,780
sympathetic projections down from your

45
00:01:33,780 --> 00:01:36,030
autonomic nervous system okay your blood

46
00:01:36,030 --> 00:01:37,860
pressure is going to rise the skin

47
00:01:37,860 --> 00:01:39,630
temperature is going to rise a little

48
00:01:39,630 --> 00:01:41,520
bit of sweat is gonna secrete from your

49
00:01:41,520 --> 00:01:42,990
sweat glands sounds like I shall have

50
00:01:42,990 --> 00:01:46,229
good ideas and it's actually your body's

51
00:01:46,229 --> 00:01:47,970
way of getting you ready to say what you

52
00:01:47,970 --> 00:01:50,009
need to say okay and in that moment

53
00:01:50,009 --> 00:01:51,899
maybe a couple of seconds later you

54
00:01:51,899 --> 00:01:53,579
might have some thoughts like well what

55
00:01:53,579 --> 00:01:55,649
if I said this and everyone thought it

56
00:01:55,649 --> 00:01:58,079
was stupid or what if I said this and

57
00:01:58,079 --> 00:02:00,090
everyone thought this isn't a great idea

58
00:02:00,090 --> 00:02:01,979
and all of a sudden I've got like brain

59
00:02:01,979 --> 00:02:05,189
regions like the periodical gray or even

60
00:02:05,189 --> 00:02:07,500
the beloved amygdala are going off at

61
00:02:07,500 --> 00:02:09,330
that time and those emotions are so

62
00:02:09,330 --> 00:02:11,430
strong that well I'm not gonna say

63
00:02:11,430 --> 00:02:13,060
anything right okay and

64
00:02:13,060 --> 00:02:15,040
in a couple of days later someone says

65
00:02:15,040 --> 00:02:16,930
it has a positive reaction in here and

66
00:02:16,930 --> 00:02:19,030
now I'm thinking about it and now I'm

67
00:02:19,030 --> 00:02:20,860
ruminating and now I'm filling really

68
00:02:20,860 --> 00:02:24,400
complex emotions like - like resentment

69
00:02:24,400 --> 00:02:27,430
or regret or even envy right and so it's

70
00:02:27,430 --> 00:02:28,840
like how do we actually bring that

71
00:02:28,840 --> 00:02:31,959
emotional journey and make it live for

72
00:02:31,959 --> 00:02:33,760
that person to understand the journey

73
00:02:33,760 --> 00:02:35,440
that they're taking with the things in

74
00:02:35,440 --> 00:02:37,180
their lives or even the relations with

75
00:02:37,180 --> 00:02:39,040
other people and so that's really what

76
00:02:39,040 --> 00:02:40,480
the affective layer is about it is about

77
00:02:40,480 --> 00:02:42,880
awareness making people more aware of

78
00:02:42,880 --> 00:02:44,650
that okay so one of the things you

79
00:02:44,650 --> 00:02:46,600
mentioned on your sites that I really

80
00:02:46,600 --> 00:02:48,130
caught my eye was like you're working at

81
00:02:48,130 --> 00:02:50,019
the intersection of affective computing

82
00:02:50,019 --> 00:02:51,850
and deep learning and so what is that

83
00:02:51,850 --> 00:02:53,920
intersection and habitat yeah I think

84
00:02:53,920 --> 00:02:56,890
that's a really good question a long

85
00:02:56,890 --> 00:02:59,830
time ago we started studying the emotion

86
00:02:59,830 --> 00:03:02,680
and really affective neuroscience which

87
00:03:02,680 --> 00:03:06,160
has been largely at first emotions were

88
00:03:06,160 --> 00:03:08,260
like oh it's a psychological construct

89
00:03:08,260 --> 00:03:11,410
and like yes sort of and then you had

90
00:03:11,410 --> 00:03:13,810
people like yak ping SAP and Antonio

91
00:03:13,810 --> 00:03:16,180
Damasio neuroscientists who showed like

92
00:03:16,180 --> 00:03:18,670
no actually these three brain regions in

93
00:03:18,670 --> 00:03:21,579
some sort of temporal fashion unveil

94
00:03:21,579 --> 00:03:23,829
when you get the emotion of something

95
00:03:23,829 --> 00:03:27,340
like seeking or rage and so they were

96
00:03:27,340 --> 00:03:29,590
able to do the necessary research to

97
00:03:29,590 --> 00:03:31,420
show that emotions were in fact real and

98
00:03:31,420 --> 00:03:33,519
they were real science and you could

99
00:03:33,519 --> 00:03:35,769
actually study them and most of that was

100
00:03:35,769 --> 00:03:37,840
done in like rodents and and then

101
00:03:37,840 --> 00:03:39,850
eventually to humans by looking at

102
00:03:39,850 --> 00:03:42,340
patients who had lesions in certain

103
00:03:42,340 --> 00:03:43,720
parts of their brain and they couldn't

104
00:03:43,720 --> 00:03:46,329
have certain types of emotions okay and

105
00:03:46,329 --> 00:03:48,880
it pointed directly to that and so the

106
00:03:48,880 --> 00:03:51,100
affective neuroscience for us the part

107
00:03:51,100 --> 00:03:53,049
is looking at the deep learning part is

108
00:03:53,049 --> 00:03:55,389
how do you understand a physiological

109
00:03:55,389 --> 00:03:57,910
signal like heart rate and heart rate is

110
00:03:57,910 --> 00:04:00,340
largely modulated by the central nervous

111
00:04:00,340 --> 00:04:02,079
by your brain right right and so what

112
00:04:02,079 --> 00:04:04,060
you want to know is what effective say

113
00:04:04,060 --> 00:04:05,290
this happening in your brain that

114
00:04:05,290 --> 00:04:07,840
eventually transmits down to you having

115
00:04:07,840 --> 00:04:10,000
a higher or a lower heart rate I see and

116
00:04:10,000 --> 00:04:12,160
so we're in trying to model that that's

117
00:04:12,160 --> 00:04:13,450
what you're modeling when you're trying

118
00:04:13,450 --> 00:04:14,920
to model emotion you're trying to model

119
00:04:14,920 --> 00:04:16,269
something that's happening in the brain

120
00:04:16,269 --> 00:04:18,250
so we thought we needed to understand

121
00:04:18,250 --> 00:04:20,590
that so that we could better model these

122
00:04:20,590 --> 00:04:22,780
emotions so that's where the crossroad

123
00:04:22,780 --> 00:04:25,150
and I think tensorflow points out a

124
00:04:25,150 --> 00:04:25,919
really good

125
00:04:25,919 --> 00:04:29,129
almost the complexity of classifying

126
00:04:29,129 --> 00:04:31,080
those emotions is you've got to have

127
00:04:31,080 --> 00:04:32,580
something like a deep learning neuron

128
00:04:32,580 --> 00:04:34,650
that and you got to have even pushing

129
00:04:34,650 --> 00:04:36,509
state-of-the-art like attention networks

130
00:04:36,509 --> 00:04:38,279
and going further and tensorflow

131
00:04:38,279 --> 00:04:39,749
provides that for us

132
00:04:39,749 --> 00:04:41,550
interesting so you've been using tensor

133
00:04:41,550 --> 00:04:45,889
flow quite a bit yes thank you

134
00:04:45,889 --> 00:04:48,870
so and so like just trying to get my

135
00:04:48,870 --> 00:04:50,819
hands around this would be that so when

136
00:04:50,819 --> 00:04:52,860
you can understand the emotions of a

137
00:04:52,860 --> 00:04:54,900
person maybe using a product that will

138
00:04:54,900 --> 00:04:56,370
help you build a better product and

139
00:04:56,370 --> 00:04:57,509
design something because you're

140
00:04:57,509 --> 00:04:59,370
reflecting their emotions all right

141
00:04:59,370 --> 00:05:02,009
right right it's like anytime you're

142
00:05:02,009 --> 00:05:03,360
trying to model something there's this

143
00:05:03,360 --> 00:05:06,900
there's this unknown right part that

144
00:05:06,900 --> 00:05:08,250
your model can't predict is why you

145
00:05:08,250 --> 00:05:10,139
can't get a hundred percent on every

146
00:05:10,139 --> 00:05:12,479
model that you put out there and a lot

147
00:05:12,479 --> 00:05:14,490
of that could be definitely subject to

148
00:05:14,490 --> 00:05:17,219
like the users emotion at that time why

149
00:05:17,219 --> 00:05:19,349
they choose to play the game at that

150
00:05:19,349 --> 00:05:21,089
time rather than another time could also

151
00:05:21,089 --> 00:05:23,250
be like their stress levels were high

152
00:05:23,250 --> 00:05:25,050
and they wanted to play video games or

153
00:05:25,050 --> 00:05:27,029
maybe the opposite but knowing that

154
00:05:27,029 --> 00:05:28,710
gives you that indicator like you're

155
00:05:28,710 --> 00:05:31,199
saying of what their state is and what

156
00:05:31,199 --> 00:05:33,629
they're motivated to do at that time so

157
00:05:33,629 --> 00:05:35,310
it's it's going to be another tool and a

158
00:05:35,310 --> 00:05:36,750
developer's toolbox to help them build

159
00:05:36,750 --> 00:05:38,879
better products exactly exactly we hope

160
00:05:38,879 --> 00:05:40,770
that everyone has access to this

161
00:05:40,770 --> 00:05:43,349
effective layer oh now I know you're

162
00:05:43,349 --> 00:05:45,060
also looking for people to help out with

163
00:05:45,060 --> 00:05:46,770
this right so could you tell us a little

164
00:05:46,770 --> 00:05:49,259
about that sure the effective layer is

165
00:05:49,259 --> 00:05:51,629
massive and if you just look at all the

166
00:05:51,629 --> 00:05:53,069
literature and affective neuroscience

167
00:05:53,069 --> 00:05:55,830
effective competing it's so much work

168
00:05:55,830 --> 00:05:57,629
has been done but yet there's so much

169
00:05:57,629 --> 00:06:00,149
left to do and we're not gonna tackle it

170
00:06:00,149 --> 00:06:03,389
alone we do wish we could say that but

171
00:06:03,389 --> 00:06:05,370
we're going to need partners and work

172
00:06:05,370 --> 00:06:07,710
with people so we think of it as there's

173
00:06:07,710 --> 00:06:09,089
people at the research end of the

174
00:06:09,089 --> 00:06:10,919
spectrum and the applications end of the

175
00:06:10,919 --> 00:06:13,500
spectrum in the research there's a lab

176
00:06:13,500 --> 00:06:16,349
out of Stanford that focuses just on

177
00:06:16,349 --> 00:06:18,539
fear right and so they're just looking

178
00:06:18,539 --> 00:06:22,080
at the physiological outputs of fear

179
00:06:22,080 --> 00:06:24,899
when you see a stimulus that scares you

180
00:06:24,899 --> 00:06:26,520
right okay and so we could actually

181
00:06:26,520 --> 00:06:29,699
partner with a lab like that or any such

182
00:06:29,699 --> 00:06:31,979
lab and say okay how do we bring this

183
00:06:31,979 --> 00:06:34,019
into deep learning and how do we make

184
00:06:34,019 --> 00:06:36,620
some sort of continuous algorithm or

185
00:06:36,620 --> 00:06:38,360
tension network that's deployed in the

186
00:06:38,360 --> 00:06:40,880
cloud to where people who have affective

187
00:06:40,880 --> 00:06:43,430
disorders with anxiety or fear anything

188
00:06:43,430 --> 00:06:45,290
like that we could be predicting when

189
00:06:45,290 --> 00:06:47,419
they're going to be fearful of what and

190
00:06:47,419 --> 00:06:48,530
then help them get better from that

191
00:06:48,530 --> 00:06:50,660
interesting and then on the application

192
00:06:50,660 --> 00:06:52,699
side I think there's a lot of developers

193
00:06:52,699 --> 00:06:54,889
to work with who want to create this

194
00:06:54,889 --> 00:06:56,960
kind of effective computing stuff but

195
00:06:56,960 --> 00:06:59,000
may not have the requisite or like

196
00:06:59,000 --> 00:07:01,490
background or studying in the affective

197
00:07:01,490 --> 00:07:03,770
or emotion sciences and there we can

198
00:07:03,770 --> 00:07:05,389
help out by saying okay how would you

199
00:07:05,389 --> 00:07:07,810
label something like discussed like

200
00:07:07,810 --> 00:07:10,400
difficult and set up the right

201
00:07:10,400 --> 00:07:12,500
experiment to be able or the labeling

202
00:07:12,500 --> 00:07:14,720
process and help with that and then the

203
00:07:14,720 --> 00:07:16,280
last part I think is like thinking of

204
00:07:16,280 --> 00:07:18,860
organizations if you want to get the

205
00:07:18,860 --> 00:07:21,680
effective like style of your team or

206
00:07:21,680 --> 00:07:23,630
something like that and so we could work

207
00:07:23,630 --> 00:07:25,699
with a company to see and it's going to

208
00:07:25,699 --> 00:07:27,710
be able to further the research on the

209
00:07:27,710 --> 00:07:29,600
affective layer but also give insights

210
00:07:29,600 --> 00:07:31,820
to whoever whoever we're working with

211
00:07:31,820 --> 00:07:33,770
right so affective computing it's a

212
00:07:33,770 --> 00:07:35,360
phrase most of us probably haven't heard

213
00:07:35,360 --> 00:07:37,430
of right now but maybe all of us will be

214
00:07:37,430 --> 00:07:38,539
using it in the not-too-distant future

215
00:07:38,539 --> 00:07:40,810
that would be great wouldn't it be cool

216
00:07:40,810 --> 00:07:42,919
thanks Christian and thanks everybody

217
00:07:42,919 --> 00:07:44,630
for watching this episode of tensorflow

218
00:07:44,630 --> 00:07:46,520
meats if you have any questions for me

219
00:07:46,520 --> 00:07:47,840
if you have any questions for Christian

220
00:07:47,840 --> 00:07:49,070
just please leave them in the comments

221
00:07:49,070 --> 00:07:50,510
below and don't forget to hit that

222
00:07:50,510 --> 00:07:57,480
subscribe button thank you

223
00:07:57,480 --> 00:07:59,540
you

