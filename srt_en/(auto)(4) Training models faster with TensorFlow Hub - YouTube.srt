1
00:00:00,590 --> 00:00:05,270
[Music]

2
00:00:05,270 --> 00:00:07,350
hi everybody we're here at the tents

3
00:00:07,350 --> 00:00:08,880
flow developer summit at the tensorflow

4
00:00:08,880 --> 00:00:11,040
cafe and I'm chatting with Andrew Gasper

5
00:00:11,040 --> 00:00:13,009
ovitch from tensor flow hub and you

6
00:00:13,009 --> 00:00:15,089
announced tensorflow hub today can you

7
00:00:15,089 --> 00:00:16,139
tell us all about it

8
00:00:16,139 --> 00:00:19,590
yeah so tensorflow hub is the new way to

9
00:00:19,590 --> 00:00:22,320
share what we're calling modules which

10
00:00:22,320 --> 00:00:25,830
is meant to be a reusable component so

11
00:00:25,830 --> 00:00:28,500
it's a little bit less than a model it's

12
00:00:28,500 --> 00:00:31,080
just the reusable portion okay but it is

13
00:00:31,080 --> 00:00:34,260
the graph the weights and potentially

14
00:00:34,260 --> 00:00:36,510
any assets that come with it so it's all

15
00:00:36,510 --> 00:00:39,120
packaged up into a component that makes

16
00:00:39,120 --> 00:00:40,829
it easy to share in just one line of

17
00:00:40,829 --> 00:00:44,489
code and one line of code yeah yeah that

18
00:00:44,489 --> 00:00:45,989
was a requirement from the beginning

19
00:00:45,989 --> 00:00:48,149
actually no more than one line of code

20
00:00:48,149 --> 00:00:49,620
where do you go from here off a lot of

21
00:00:49,620 --> 00:00:52,280
code yeah yeah well I guess the natural

22
00:00:52,280 --> 00:00:57,000
zero that's next year now one of the

23
00:00:57,000 --> 00:00:58,230
things you mentioned in your talk was

24
00:00:58,230 --> 00:01:00,180
about these modules being composable

25
00:01:00,180 --> 00:01:02,460
reusable and retrain of all could yes

26
00:01:02,460 --> 00:01:04,290
that's true that composable for us yeah

27
00:01:04,290 --> 00:01:08,040
so composable just means that you can do

28
00:01:08,040 --> 00:01:10,710
things like add your own classification

29
00:01:10,710 --> 00:01:13,080
so if you're talking about an image mod

30
00:01:13,080 --> 00:01:16,640
module it doesn't include the

31
00:01:16,640 --> 00:01:18,930
classification from the model so that is

32
00:01:18,930 --> 00:01:21,720
something that you can compose with what

33
00:01:21,720 --> 00:01:25,680
you're building in the case of text

34
00:01:25,680 --> 00:01:27,299
classification that may be something

35
00:01:27,299 --> 00:01:29,909
like an embedding module okay and then

36
00:01:29,909 --> 00:01:31,680
in other cases we also want to have just

37
00:01:31,680 --> 00:01:35,100
general-purpose modules that you can

38
00:01:35,100 --> 00:01:38,189
compose around almost like functions

39
00:01:38,189 --> 00:01:41,670
that you can call together and make

40
00:01:41,670 --> 00:01:44,970
building blocks for an entire model I

41
00:01:44,970 --> 00:01:46,860
say so so if I have a model that I don't

42
00:01:46,860 --> 00:01:48,420
know for example of doing OCR in an

43
00:01:48,420 --> 00:01:51,509
image and but it can also tag things

44
00:01:51,509 --> 00:01:53,340
within that image what saying is I could

45
00:01:53,340 --> 00:01:55,259
break out the OCR part and I could break

46
00:01:55,259 --> 00:01:56,610
out the tagging part and compose them

47
00:01:56,610 --> 00:01:57,750
into something new yeah absolutely

48
00:01:57,750 --> 00:01:59,939
absolutely and that's gonna be something

49
00:01:59,939 --> 00:02:02,009
really interesting to see what develops

50
00:02:02,009 --> 00:02:04,350
in the community over time just the ways

51
00:02:04,350 --> 00:02:06,930
of putting things together and sort of

52
00:02:06,930 --> 00:02:09,179
modules that people end up ryeol yeah so

53
00:02:09,179 --> 00:02:10,770
it's we're providing building all the

54
00:02:10,770 --> 00:02:12,470
communities providing building blocks

55
00:02:12,470 --> 00:02:13,710
yes

56
00:02:13,710 --> 00:02:17,520
we have we have a number of modules to

57
00:02:17,520 --> 00:02:18,720
start with and they're very

58
00:02:18,720 --> 00:02:21,180
general-purpose things doing you know

59
00:02:21,180 --> 00:02:24,120
image classification doing text

60
00:02:24,120 --> 00:02:25,890
classification we have those embedding

61
00:02:25,890 --> 00:02:29,520
modules but I think that over time what

62
00:02:29,520 --> 00:02:31,500
the community can contribute is what

63
00:02:31,500 --> 00:02:33,750
really will be interesting sounds great

64
00:02:33,750 --> 00:02:35,550
so we've mentioned that there composable

65
00:02:35,550 --> 00:02:39,090
and then reusable reusable just means

66
00:02:39,090 --> 00:02:42,600
that you can basically take something

67
00:02:42,600 --> 00:02:45,570
that already exists and apply it to your

68
00:02:45,570 --> 00:02:47,400
own particular problem makes perfect

69
00:02:47,400 --> 00:02:49,050
sense yeah and then I'm really

70
00:02:49,050 --> 00:02:50,910
interested in retraining below I mean we

71
00:02:50,910 --> 00:02:52,620
speak a lot about the attentive love for

72
00:02:52,620 --> 00:02:54,120
poets where you can retrain the final

73
00:02:54,120 --> 00:02:55,950
layer to make it flowers instead of

74
00:02:55,950 --> 00:02:58,170
general damages and is it to do a

75
00:02:58,170 --> 00:03:00,510
similar thing Gaudi yes yeah definitely

76
00:03:00,510 --> 00:03:03,150
so there is you know the classic sort of

77
00:03:03,150 --> 00:03:05,310
transfer learning case there where you

78
00:03:05,310 --> 00:03:06,720
would take an image classification

79
00:03:06,720 --> 00:03:09,600
module just up to the feature vectors

80
00:03:09,600 --> 00:03:12,180
and then retrain your own classification

81
00:03:12,180 --> 00:03:15,090
on top to do things but you know you can

82
00:03:15,090 --> 00:03:17,550
also go deeper than that you can start

83
00:03:17,550 --> 00:03:20,640
retraining text embedding modules and

84
00:03:20,640 --> 00:03:23,340
the thing about TF hub is that you can

85
00:03:23,340 --> 00:03:26,190
actually do fine-tuning and Retraining

86
00:03:26,190 --> 00:03:28,980
inside the module itself so if you have

87
00:03:28,980 --> 00:03:31,250
enough data you can actually turn on

88
00:03:31,250 --> 00:03:33,750
retraining inside the module because

89
00:03:33,750 --> 00:03:36,330
it's just a graph with the weights so

90
00:03:36,330 --> 00:03:39,840
you know you can really get better

91
00:03:39,840 --> 00:03:42,900
results because it's not just something

92
00:03:42,900 --> 00:03:44,910
that's static it's something that you

93
00:03:44,910 --> 00:03:47,190
can really include in your own model

94
00:03:47,190 --> 00:03:48,990
sounds good now in your talk you showed

95
00:03:48,990 --> 00:03:51,450
this for like classifying rabbits yes

96
00:03:51,450 --> 00:03:53,460
yeah yeah and you had that one picture

97
00:03:53,460 --> 00:03:54,450
of a rabbit which I think is the best

98
00:03:54,450 --> 00:03:57,360
picture of any in the entire day so we

99
00:03:57,360 --> 00:03:59,040
have to cut to that and show it's a

100
00:03:59,040 --> 00:04:01,830
beautiful yeah so tell us what did you

101
00:04:01,830 --> 00:04:04,680
do in that demo in the demo we basically

102
00:04:04,680 --> 00:04:06,960
just it's the same ideas the tensorflow

103
00:04:06,960 --> 00:04:10,260
for poets we start with a general

104
00:04:10,260 --> 00:04:12,750
purpose image classification model just

105
00:04:12,750 --> 00:04:15,900
up to the feature vectors and in the

106
00:04:15,900 --> 00:04:17,700
particular demo example we were

107
00:04:17,700 --> 00:04:19,920
classifying rabbits maybe including the

108
00:04:19,920 --> 00:04:22,260
Easter Bunny and you know we added our

109
00:04:22,260 --> 00:04:25,650
own classification on top and we fed in

110
00:04:25,650 --> 00:04:28,680
all of our own training examples and the

111
00:04:28,680 --> 00:04:30,360
end result is that you get something

112
00:04:30,360 --> 00:04:33,449
that is special to your task but

113
00:04:33,449 --> 00:04:35,310
includes all of the benefits of the

114
00:04:35,310 --> 00:04:37,380
general-purpose model nice it's kind of

115
00:04:37,380 --> 00:04:39,660
fun yeah how was accurate was at

116
00:04:39,660 --> 00:04:42,600
detecting rabbits it's very accurate

117
00:04:42,600 --> 00:04:44,400
we'll have to see how it works with the

118
00:04:44,400 --> 00:04:46,620
Easter Bunny but you didn't put the

119
00:04:46,620 --> 00:04:50,580
Easter Bunny in your test next year so

120
00:04:50,580 --> 00:04:52,289
so if I'm a developer and I want to

121
00:04:52,289 --> 00:04:54,210
build like I'm maybe I'm an expert in

122
00:04:54,210 --> 00:04:56,280
rabbit detection and you know and I want

123
00:04:56,280 --> 00:04:58,770
to build like a custom model and I want

124
00:04:58,770 --> 00:05:00,240
to contribute this into tensorflow hub

125
00:05:00,240 --> 00:05:01,650
how would I go about doing that well

126
00:05:01,650 --> 00:05:03,180
that's something that we're really

127
00:05:03,180 --> 00:05:05,220
excited to work on over the next few

128
00:05:05,220 --> 00:05:08,490
months right now a module is accessible

129
00:05:08,490 --> 00:05:11,789
via any URL and so you can host a module

130
00:05:11,789 --> 00:05:13,830
and you can use it in that one line of

131
00:05:13,830 --> 00:05:15,960
code but we want to create a platform

132
00:05:15,960 --> 00:05:19,349
where people can go and find modules for

133
00:05:19,349 --> 00:05:21,120
all sorts of different topics and have

134
00:05:21,120 --> 00:05:23,520
that one central place that you always

135
00:05:23,520 --> 00:05:24,930
know that you can get really high

136
00:05:24,930 --> 00:05:26,820
quality stuff in a variety of different

137
00:05:26,820 --> 00:05:29,070
areas and is there like some kind of

138
00:05:29,070 --> 00:05:32,250
curation that's something it remains to

139
00:05:32,250 --> 00:05:34,650
be you know seeing specifically what

140
00:05:34,650 --> 00:05:37,380
we're gonna do but we want to be able to

141
00:05:37,380 --> 00:05:39,660
open it up to it as wide a group of

142
00:05:39,660 --> 00:05:41,880
people as possible sounds good so where

143
00:05:41,880 --> 00:05:42,780
can I find this again

144
00:05:42,780 --> 00:05:46,169
it's tensorflow org slash hub tensorflow

145
00:05:46,169 --> 00:05:48,090
org slash hub I keep saying tensorflow

146
00:05:48,090 --> 00:05:53,190
hub / org we should remain tensorflow

147
00:05:53,190 --> 00:05:55,199
org slash hub yeah you know go check it

148
00:05:55,199 --> 00:05:57,120
out so thank you so much Andrew thank

149
00:05:57,120 --> 00:05:59,039
you it's been really fun and I love that

150
00:05:59,039 --> 00:06:02,340
slide okay so thanks everybody for

151
00:06:02,340 --> 00:06:03,840
watching this episode if you got any

152
00:06:03,840 --> 00:06:05,039
questions for me or if you have any

153
00:06:05,039 --> 00:06:06,720
questions for Andrew please leave them

154
00:06:06,720 --> 00:06:08,159
in the comments below and all of the

155
00:06:08,159 --> 00:06:09,750
links that we spoke about today will put

156
00:06:09,750 --> 00:06:11,400
in the text description so thank you so

157
00:06:11,400 --> 00:06:12,750
much and don't forget to hit that

158
00:06:12,750 --> 00:06:14,380
subscribe button

159
00:06:14,380 --> 00:06:30,960
[Music]

160
00:06:30,960 --> 00:06:33,020
you

