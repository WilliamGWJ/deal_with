1
00:00:02,880 --> 00:00:06,470
[Music]

2
00:00:06,470 --> 00:00:09,510
my name is Priya and I'm Anjali

3
00:00:09,510 --> 00:00:11,309
we're both software engineers on the

4
00:00:11,309 --> 00:00:13,920
tensorflow team working on distributed

5
00:00:13,920 --> 00:00:16,980
tensorflow we're so excited to be here

6
00:00:16,980 --> 00:00:18,990
today to tell you about distributor

7
00:00:18,990 --> 00:00:20,250
tensorflow training

8
00:00:20,250 --> 00:00:27,060
let me grab the clicker okay hopefully

9
00:00:27,060 --> 00:00:29,699
most of you know what tensorflow is it's

10
00:00:29,699 --> 00:00:31,349
an open-source machine learning

11
00:00:31,349 --> 00:00:34,500
framework used extensively both inside

12
00:00:34,500 --> 00:00:38,550
and outside Google for example if you

13
00:00:38,550 --> 00:00:40,500
tried the smart compose feature that was

14
00:00:40,500 --> 00:00:42,600
launched a couple of days ago that

15
00:00:42,600 --> 00:00:47,820
feature uses tensorflow tensorflow

16
00:00:47,820 --> 00:00:50,700
allows you to build train and predict

17
00:00:50,700 --> 00:00:54,059
using neural networks such as this in

18
00:00:54,059 --> 00:00:56,940
training we learn the parameters of the

19
00:00:56,940 --> 00:01:02,399
network using data training complex

20
00:01:02,399 --> 00:01:04,470
neural networks with large amounts of

21
00:01:04,470 --> 00:01:07,799
data can often take a long time in the

22
00:01:07,799 --> 00:01:10,200
graph here you can see you can see the

23
00:01:10,200 --> 00:01:12,780
training time on the x-axis and the

24
00:01:12,780 --> 00:01:15,350
accuracy of predictions on the y-axis

25
00:01:15,350 --> 00:01:18,150
this is taken from training an image

26
00:01:18,150 --> 00:01:22,080
recognition model on a single GPU as you

27
00:01:22,080 --> 00:01:24,630
can see it took more than 80 hours to

28
00:01:24,630 --> 00:01:28,619
get to 75% accuracy if you have some

29
00:01:28,619 --> 00:01:30,299
experience running complex machine

30
00:01:30,299 --> 00:01:32,460
learning models this might sound rather

31
00:01:32,460 --> 00:01:35,670
familiar to you and it might make you

32
00:01:35,670 --> 00:01:39,930
feel something like this if your

33
00:01:39,930 --> 00:01:41,850
training takes only a few minutes to a

34
00:01:41,850 --> 00:01:44,610
few hours you'll be productive and happy

35
00:01:44,610 --> 00:01:48,020
and you can try out new ideas faster

36
00:01:48,020 --> 00:01:51,720
when it starts to take a few days maybe

37
00:01:51,720 --> 00:01:53,310
you could still manage and run a few

38
00:01:53,310 --> 00:01:56,610
things in parallel when it starts to

39
00:01:56,610 --> 00:01:59,280
take a few weeks your progress will slow

40
00:01:59,280 --> 00:02:02,219
down and it becomes expensive to try out

41
00:02:02,219 --> 00:02:06,360
every new idea and when it starts to

42
00:02:06,360 --> 00:02:08,220
take more than a month I think it's not

43
00:02:08,220 --> 00:02:11,610
even worth thinking about and this is

44
00:02:11,610 --> 00:02:13,250
not an exaggeration

45
00:02:13,250 --> 00:02:15,630
training complex models such as the

46
00:02:15,630 --> 00:02:17,220
resonate 50 that we'll talk about later

47
00:02:17,220 --> 00:02:20,220
in the talk can take up to a week on a

48
00:02:20,220 --> 00:02:23,400
single but powerful GPU like a Tesla P

49
00:02:23,400 --> 00:02:29,250
100 so natural question to us is how can

50
00:02:29,250 --> 00:02:31,980
we make training fast there are a number

51
00:02:31,980 --> 00:02:34,709
of things you can try you can use a

52
00:02:34,709 --> 00:02:37,500
faster accelerator such as a TPU or

53
00:02:37,500 --> 00:02:39,930
tensor processing units I'm sure you've

54
00:02:39,930 --> 00:02:41,459
heard all about them in the last couple

55
00:02:41,459 --> 00:02:44,880
of days here your input pipeline might

56
00:02:44,880 --> 00:02:46,860
be the ball leg so you can work on that

57
00:02:46,860 --> 00:02:49,860
and make that faster there are a number

58
00:02:49,860 --> 00:02:52,290
of guidelines on the tensorflow website

59
00:02:52,290 --> 00:02:53,760
that you can try and improve the

60
00:02:53,760 --> 00:02:57,420
performance or your training in this

61
00:02:57,420 --> 00:03:00,350
talk will focus on distributed training

62
00:03:00,350 --> 00:03:03,540
that is running training in parallel on

63
00:03:03,540 --> 00:03:07,680
multiple devices such as CPUs GPUs or TP

64
00:03:07,680 --> 00:03:09,570
use in order to make your training

65
00:03:09,570 --> 00:03:13,500
faster with the techniques that we'll

66
00:03:13,500 --> 00:03:15,570
talk about in this talk you can bring

67
00:03:15,570 --> 00:03:17,910
down your training time from weeks two

68
00:03:17,910 --> 00:03:20,160
hours with just a few lines of code and

69
00:03:20,160 --> 00:03:23,790
a few powerful GPUs in the graph here

70
00:03:23,790 --> 00:03:26,190
you can see the images per second

71
00:03:26,190 --> 00:03:28,620
processed while training an image

72
00:03:28,620 --> 00:03:32,070
recognition model as you can see as we

73
00:03:32,070 --> 00:03:34,709
increase the number of GPUs from one to

74
00:03:34,709 --> 00:03:37,560
four to eight the images per second

75
00:03:37,560 --> 00:03:40,370
process it can almost double every time

76
00:03:40,370 --> 00:03:42,540
we'll come back to these performance

77
00:03:42,540 --> 00:03:46,190
numbers later with more details

78
00:03:46,190 --> 00:03:48,989
so before diving into the details of how

79
00:03:48,989 --> 00:03:50,820
you can get that kind of scaling and

80
00:03:50,820 --> 00:03:51,450
tensorflow

81
00:03:51,450 --> 00:03:54,030
first I want to cover a few high-level

82
00:03:54,030 --> 00:03:55,799
concepts and architectures in

83
00:03:55,799 --> 00:03:58,950
distributed training this will give us a

84
00:03:58,950 --> 00:04:00,900
strong foundation with which to

85
00:04:00,900 --> 00:04:05,610
understand the various solutions as your

86
00:04:05,610 --> 00:04:07,920
focus on training today let's take a

87
00:04:07,920 --> 00:04:10,410
look at what a typical training loop

88
00:04:10,410 --> 00:04:14,340
looks like let's say you have a simple

89
00:04:14,340 --> 00:04:16,769
model like this with a couple of hidden

90
00:04:16,769 --> 00:04:19,530
layers each layer has a bunch of weights

91
00:04:19,530 --> 00:04:22,289
and biases also called the model

92
00:04:22,289 --> 00:04:24,920
parameters or trainable variables

93
00:04:24,920 --> 00:04:29,060
a training step begins with some

94
00:04:29,060 --> 00:04:31,670
processing on the input data we then

95
00:04:31,670 --> 00:04:33,940
feed this input into the model and

96
00:04:33,940 --> 00:04:36,380
compute the predictions in the forward

97
00:04:36,380 --> 00:04:40,010
pass we then compare the predictions

98
00:04:40,010 --> 00:04:42,080
with the input label and compare to

99
00:04:42,080 --> 00:04:45,500
compute the loss then in the backward

100
00:04:45,500 --> 00:04:47,390
pass we compute the gradients and

101
00:04:47,390 --> 00:04:50,420
finally we update the model's parameters

102
00:04:50,420 --> 00:04:53,300
using these gradients this whole process

103
00:04:53,300 --> 00:04:56,870
is known as one training step and the

104
00:04:56,870 --> 00:04:59,240
training loop repeats this training step

105
00:04:59,240 --> 00:05:04,240
until you reach the desired accuracy

106
00:05:04,240 --> 00:05:06,800
let's say you begin your training with a

107
00:05:06,800 --> 00:05:09,050
simple machine under your desk with a

108
00:05:09,050 --> 00:05:12,020
multi-core CPU luckily tensorflow

109
00:05:12,020 --> 00:05:14,630
handles scaling onto a multi-core CPU

110
00:05:14,630 --> 00:05:19,250
for you automatically next you may speed

111
00:05:19,250 --> 00:05:21,500
up by a dagger accelerator to your

112
00:05:21,500 --> 00:05:25,220
machine such as a GPU or a TPU with

113
00:05:25,220 --> 00:05:27,290
distributed training you can go even

114
00:05:27,290 --> 00:05:30,440
further you can go from one machine with

115
00:05:30,440 --> 00:05:32,900
a single device to one machine with

116
00:05:32,900 --> 00:05:36,530
multiple devices and finally to multiple

117
00:05:36,530 --> 00:05:38,990
machines with possibly multiple devices

118
00:05:38,990 --> 00:05:43,160
each connected over the network with

119
00:05:43,160 --> 00:05:45,020
little with a number of techniques

120
00:05:45,020 --> 00:05:47,240
eventually it's possible to scale to

121
00:05:47,240 --> 00:05:49,730
hundreds of devices and that's indeed

122
00:05:49,730 --> 00:05:53,210
what we do in a lot of Google systems by

123
00:05:53,210 --> 00:05:54,980
the way in the rest of this talk we'll

124
00:05:54,980 --> 00:05:57,350
use the terms device worker or

125
00:05:57,350 --> 00:06:00,290
accelerator to refer to processing units

126
00:06:00,290 --> 00:06:04,490
such as GPUs or GPUs so how does

127
00:06:04,490 --> 00:06:07,220
distributor training work like

128
00:06:07,220 --> 00:06:08,450
everything else in software engineering

129
00:06:08,450 --> 00:06:10,280
there are a number of ways to go about

130
00:06:10,280 --> 00:06:12,020
when you think about distributing your

131
00:06:12,020 --> 00:06:17,000
training what approach you pick depends

132
00:06:17,000 --> 00:06:19,430
on the size of your model the amount of

133
00:06:19,430 --> 00:06:22,340
training data you have and the available

134
00:06:22,340 --> 00:06:25,240
devices

135
00:06:25,240 --> 00:06:27,500
the most common architecture in

136
00:06:27,500 --> 00:06:29,690
distributed training and what is what is

137
00:06:29,690 --> 00:06:33,110
known as data parallelism in data

138
00:06:33,110 --> 00:06:35,900
parallelism we run the same model and

139
00:06:35,900 --> 00:06:36,490
compute

140
00:06:36,490 --> 00:06:38,979
on each worker but with a different

141
00:06:38,979 --> 00:06:41,949
slice of the input data each device

142
00:06:41,949 --> 00:06:45,039
computes the loss and the gradients we

143
00:06:45,039 --> 00:06:47,020
use these gradients to update the models

144
00:06:47,020 --> 00:06:50,139
parameters and the updated model is then

145
00:06:50,139 --> 00:06:52,979
used in the next round of computation

146
00:06:52,979 --> 00:06:55,419
there are two common approaches when you

147
00:06:55,419 --> 00:06:57,220
think about how do you update the model

148
00:06:57,220 --> 00:07:02,259
using these gradients the first approach

149
00:07:02,259 --> 00:07:04,389
is what is known as a synchronous

150
00:07:04,389 --> 00:07:07,780
parameter server approach in this

151
00:07:07,780 --> 00:07:11,650
approach we designate some some devices

152
00:07:11,650 --> 00:07:14,349
as parameter servers as shown in blue

153
00:07:14,349 --> 00:07:17,800
here these servers hold the parameters

154
00:07:17,800 --> 00:07:21,039
of your model others are designated as

155
00:07:21,039 --> 00:07:25,150
workers as shown in green here workers

156
00:07:25,150 --> 00:07:28,120
do the bulk of the computation each

157
00:07:28,120 --> 00:07:30,340
worker fetches the parameters from the

158
00:07:30,340 --> 00:07:32,919
parameter server it then confused the

159
00:07:32,919 --> 00:07:35,800
last ingredients it sends the gradients

160
00:07:35,800 --> 00:07:38,470
back to the parameter server which then

161
00:07:38,470 --> 00:07:40,870
updates the models parameters using

162
00:07:40,870 --> 00:07:45,490
these gradients each worker does this

163
00:07:45,490 --> 00:07:48,190
independently so this allows us to scale

164
00:07:48,190 --> 00:07:49,990
this approach to a large number of

165
00:07:49,990 --> 00:07:53,349
workers this has worked well for many

166
00:07:53,349 --> 00:07:55,930
models in Google where training workers

167
00:07:55,930 --> 00:07:58,360
might be preempted by high priority

168
00:07:58,360 --> 00:08:01,120
production jobs or where this asymmetry

169
00:08:01,120 --> 00:08:03,699
between the different workers or where

170
00:08:03,699 --> 00:08:05,650
machines might go down for regular

171
00:08:05,650 --> 00:08:08,919
maintenance and all of this doesn't hurt

172
00:08:08,919 --> 00:08:10,960
the scaling because the workers are not

173
00:08:10,960 --> 00:08:14,530
beating on each other the downside of

174
00:08:14,530 --> 00:08:16,840
this approach however is that workers

175
00:08:16,840 --> 00:08:19,479
can get out of sync their computing

176
00:08:19,479 --> 00:08:21,430
their gradients on steel parameter

177
00:08:21,430 --> 00:08:26,520
values and this can delay convergence

178
00:08:26,520 --> 00:08:29,469
the second approach is what is known as

179
00:08:29,469 --> 00:08:32,649
synchronous all reduce this approach has

180
00:08:32,649 --> 00:08:35,140
become more common with the rise of fast

181
00:08:35,140 --> 00:08:38,969
accelerators such as keep use or GPUs in

182
00:08:38,969 --> 00:08:42,820
this approach each worker has a copy of

183
00:08:42,820 --> 00:08:44,860
parameters on its own the

184
00:08:44,860 --> 00:08:48,339
no special parameter servers each worker

185
00:08:48,339 --> 00:08:51,220
confused the lost ingredients based on a

186
00:08:51,220 --> 00:08:54,730
subset of training samples once the

187
00:08:54,730 --> 00:08:56,950
gradients are computed the workers

188
00:08:56,950 --> 00:08:58,630
communicate among themselves to

189
00:08:58,630 --> 00:09:01,540
propagate the gradients and update their

190
00:09:01,540 --> 00:09:05,050
model parameters all the workers are

191
00:09:05,050 --> 00:09:07,570
synchronized which means that the next

192
00:09:07,570 --> 00:09:09,760
round of computation doesn't begin until

193
00:09:09,760 --> 00:09:12,640
each worker has received the updated

194
00:09:12,640 --> 00:09:15,540
gradients and a barrier that's Mauro

195
00:09:15,540 --> 00:09:18,220
when you have fast devices in a

196
00:09:18,220 --> 00:09:21,040
controlled environment the variance of

197
00:09:21,040 --> 00:09:22,899
step time between the different workers

198
00:09:22,899 --> 00:09:26,680
can be small when combined with strong

199
00:09:26,680 --> 00:09:28,450
communication links between the

200
00:09:28,450 --> 00:09:31,899
different devices overall overhead of

201
00:09:31,899 --> 00:09:35,019
synchronization can be small so whenever

202
00:09:35,019 --> 00:09:37,690
practical this approach can lead to

203
00:09:37,690 --> 00:09:43,899
faster convergence a class of algorithms

204
00:09:43,899 --> 00:09:45,640
called all reduce can be used to

205
00:09:45,640 --> 00:09:47,890
efficiently combine the gradients across

206
00:09:47,890 --> 00:09:50,589
the different workers all reduce

207
00:09:50,589 --> 00:09:52,269
aggregates the values from different

208
00:09:52,269 --> 00:09:54,940
workers for example by adding them up

209
00:09:54,940 --> 00:09:57,820
and then copying them to the different

210
00:09:57,820 --> 00:10:01,600
workers it's a fuse algorithm that can

211
00:10:01,600 --> 00:10:04,029
be very efficient and it can reduce the

212
00:10:04,029 --> 00:10:06,459
overhead of synchronization of gradients

213
00:10:06,459 --> 00:10:08,430
by a lot

214
00:10:08,430 --> 00:10:10,930
there are many all reduce algorithms

215
00:10:10,930 --> 00:10:13,050
available depending on the type of

216
00:10:13,050 --> 00:10:15,310
communication available between the

217
00:10:15,310 --> 00:10:19,120
different workers one common algorithm

218
00:10:19,120 --> 00:10:21,720
is what is known as Ringold reduce in

219
00:10:21,720 --> 00:10:25,360
recoil reduce each worker sends his

220
00:10:25,360 --> 00:10:27,399
gradients to a successor on the ring and

221
00:10:27,399 --> 00:10:30,630
receives gradients from its predecessor

222
00:10:30,630 --> 00:10:32,829
there are a few more such rounds of

223
00:10:32,829 --> 00:10:34,779
gradient exchanges I won't be going into

224
00:10:34,779 --> 00:10:37,540
the details here but at the end of the

225
00:10:37,540 --> 00:10:40,390
algorithm each worker has received a

226
00:10:40,390 --> 00:10:44,320
copy of the combined gradients ring all

227
00:10:44,320 --> 00:10:46,959
reduce uses network bandwidth optimally

228
00:10:46,959 --> 00:10:49,690
because it uses both the upload and the

229
00:10:49,690 --> 00:10:53,230
download bandwidth at each worker it can

230
00:10:53,230 --> 00:10:56,079
also overlap the gradient computation at

231
00:10:56,079 --> 00:10:57,530
lower layers in the net

232
00:10:57,530 --> 00:10:59,690
with transmission of radiance at the

233
00:10:59,690 --> 00:11:02,240
higher layer which means it can further

234
00:11:02,240 --> 00:11:06,110
reduce the training time wrinkle reduce

235
00:11:06,110 --> 00:11:08,750
is just one approach and some hardware

236
00:11:08,750 --> 00:11:10,580
vendors supplies specialized

237
00:11:10,580 --> 00:11:12,860
implementations of all reduce for their

238
00:11:12,860 --> 00:11:15,580
hardware for example the NVIDIA nickel

239
00:11:15,580 --> 00:11:18,530
we have a team in Google working on fast

240
00:11:18,530 --> 00:11:20,780
implementations of all reduce for

241
00:11:20,780 --> 00:11:24,170
various device topologies the bottom

242
00:11:24,170 --> 00:11:26,690
line is that all reduce can be fast when

243
00:11:26,690 --> 00:11:28,640
working with multiple devices on a

244
00:11:28,640 --> 00:11:31,280
single machine or multiple image or a

245
00:11:31,280 --> 00:11:36,170
small number of machines so given these

246
00:11:36,170 --> 00:11:37,730
two broad architectures and data

247
00:11:37,730 --> 00:11:40,100
parallelism you may be wondering which

248
00:11:40,100 --> 00:11:42,800
approach should you pick there isn't one

249
00:11:42,800 --> 00:11:45,800
right answer parameter server approach

250
00:11:45,800 --> 00:11:47,780
is preferable if you have a large number

251
00:11:47,780 --> 00:11:50,840
of not so powerful or not so reliable

252
00:11:50,840 --> 00:11:53,480
machines for example if you have a large

253
00:11:53,480 --> 00:11:57,230
cluster of machines with just CPUs the

254
00:11:57,230 --> 00:11:58,940
synchronous already's approach on the

255
00:11:58,940 --> 00:12:00,830
other hand is preferable if you have

256
00:12:00,830 --> 00:12:03,290
fast devices with strong communication

257
00:12:03,290 --> 00:12:07,280
links such as TP use or multiple GPUs on

258
00:12:07,280 --> 00:12:10,310
a single machine parameter server

259
00:12:10,310 --> 00:12:12,230
approach has been around for a while and

260
00:12:12,230 --> 00:12:14,410
it has been supported well in tensorflow

261
00:12:14,410 --> 00:12:17,300
Tipu's on the other hand use already

262
00:12:17,300 --> 00:12:19,580
would all reduce approach out of the box

263
00:12:19,580 --> 00:12:22,640
in the next section of this talk we'll

264
00:12:22,640 --> 00:12:24,740
show you how you can scale your training

265
00:12:24,740 --> 00:12:26,960
using the all reduce approach on

266
00:12:26,960 --> 00:12:29,540
multiple GPUs with just a few lines of

267
00:12:29,540 --> 00:12:35,210
code before I get into that I just want

268
00:12:35,210 --> 00:12:37,610
to mention another type of distributed

269
00:12:37,610 --> 00:12:39,650
training known as model parallelism that

270
00:12:39,650 --> 00:12:42,710
you may have heard of a simple way to

271
00:12:42,710 --> 00:12:45,050
think about model parallelism is when

272
00:12:45,050 --> 00:12:47,300
your model is so big that it doesn't fit

273
00:12:47,300 --> 00:12:50,390
in the memory of one device so you

274
00:12:50,390 --> 00:12:52,700
divide the model into smaller parts and

275
00:12:52,700 --> 00:12:54,860
you can do those computations on

276
00:12:54,860 --> 00:12:57,320
different workers with the same training

277
00:12:57,320 --> 00:13:00,500
samples for example you could put

278
00:13:00,500 --> 00:13:02,030
different layers of your model on

279
00:13:02,030 --> 00:13:05,270
different devices these days however

280
00:13:05,270 --> 00:13:08,690
most devices have big enough memory that

281
00:13:08,690 --> 00:13:10,730
most models can fit in their memory

282
00:13:10,730 --> 00:13:12,980
so in the rest of this talk we'll

283
00:13:12,980 --> 00:13:18,400
continue to focus on data parallelism

284
00:13:18,400 --> 00:13:20,960
now that you're armed with fundamentals

285
00:13:20,960 --> 00:13:22,760
of distributed training architectures

286
00:13:22,760 --> 00:13:25,010
let's see how you can do this intensive

287
00:13:25,010 --> 00:13:27,350
flow as I already mentioned we're going

288
00:13:27,350 --> 00:13:29,890
to focus on scaling to multiple GPUs

289
00:13:29,890 --> 00:13:35,360
with the all reduce architecture in

290
00:13:35,360 --> 00:13:37,610
order to do so easily I'm pleased to

291
00:13:37,610 --> 00:13:39,710
introduce a new distribution strategy

292
00:13:39,710 --> 00:13:43,250
API this API allows you to distribute

293
00:13:43,250 --> 00:13:45,950
your training intensive flow with very

294
00:13:45,950 --> 00:13:49,700
little modification to your code with

295
00:13:49,700 --> 00:13:52,220
distribution strategy API you no longer

296
00:13:52,220 --> 00:13:55,070
need to place your ops or parameters on

297
00:13:55,070 --> 00:13:58,460
specific devices you don't need to worry

298
00:13:58,460 --> 00:14:00,500
about structuring your model in a way

299
00:14:00,500 --> 00:14:03,020
that the gradients or losses across

300
00:14:03,020 --> 00:14:06,100
devices are aggregated correctly

301
00:14:06,100 --> 00:14:08,750
distribution does so distributions Riley

302
00:14:08,750 --> 00:14:10,010
does that for you

303
00:14:10,010 --> 00:14:13,660
it is easy to use and fast to Train

304
00:14:13,660 --> 00:14:16,580
now let's look at some code to see how

305
00:14:16,580 --> 00:14:18,500
you can do this intense how we can use

306
00:14:18,500 --> 00:14:23,150
this API in our example we're going to

307
00:14:23,150 --> 00:14:25,550
be using tens of flows high-level API

308
00:14:25,550 --> 00:14:28,400
call estimator if you use this API

309
00:14:28,400 --> 00:14:31,160
before you might be familiar with the

310
00:14:31,160 --> 00:14:33,560
following snippet of code to create a

311
00:14:33,560 --> 00:14:37,190
custom estimator it requires three

312
00:14:37,190 --> 00:14:40,400
arguments the first one is a function

313
00:14:40,400 --> 00:14:43,760
that defines your model so it defines

314
00:14:43,760 --> 00:14:45,980
the parameters of your model how you

315
00:14:45,980 --> 00:14:47,960
compute the loss and the gradients and

316
00:14:47,960 --> 00:14:51,260
how you update the models parameters the

317
00:14:51,260 --> 00:14:53,450
second argument is the directory where

318
00:14:53,450 --> 00:14:55,340
you want to persist the state of your

319
00:14:55,340 --> 00:14:58,550
model and the third argument is a

320
00:14:58,550 --> 00:15:01,130
configuration called run config where

321
00:15:01,130 --> 00:15:03,710
you can specify things like how often

322
00:15:03,710 --> 00:15:06,050
you want to checkpoint how often

323
00:15:06,050 --> 00:15:08,960
summaries should be saved and so on in

324
00:15:08,960 --> 00:15:10,580
this case we've used the default run

325
00:15:10,580 --> 00:15:14,600
config once you create the estimator you

326
00:15:14,600 --> 00:15:16,610
can start your training by calling the

327
00:15:16,610 --> 00:15:17,490
train method

328
00:15:17,490 --> 00:15:19,950
with the input function that provides

329
00:15:19,950 --> 00:15:21,950
your training data

330
00:15:21,950 --> 00:15:25,770
so given this code to do the training on

331
00:15:25,770 --> 00:15:28,770
one device how can you change it to run

332
00:15:28,770 --> 00:15:32,910
on multiple GPUs you simply need to add

333
00:15:32,910 --> 00:15:36,209
one line of code instantiate something

334
00:15:36,209 --> 00:15:39,240
called mirrored strategy and pass it to

335
00:15:39,240 --> 00:15:42,839
the run config call that's it that's all

336
00:15:42,839 --> 00:15:44,790
the code changes you need to scale this

337
00:15:44,790 --> 00:15:48,420
code to multiple GPUs mirrored strategy

338
00:15:48,420 --> 00:15:50,760
is the type of distribution strategy api

339
00:15:50,760 --> 00:15:53,910
that i just mentioned with this api you

340
00:15:53,910 --> 00:15:55,709
don't need to make any changes to your

341
00:15:55,709 --> 00:15:58,529
model function or your input function or

342
00:15:58,529 --> 00:16:01,020
your training loop you don't even need

343
00:16:01,020 --> 00:16:03,779
to specify your devices if you want to

344
00:16:03,779 --> 00:16:06,209
run on all available devices it will

345
00:16:06,209 --> 00:16:08,310
automatically detect that and run your

346
00:16:08,310 --> 00:16:11,670
training on all available GPUs so that's

347
00:16:11,670 --> 00:16:13,320
it those are all the code changes you

348
00:16:13,320 --> 00:16:17,220
need this API is available in TF contrib

349
00:16:17,220 --> 00:16:19,740
and you can use it you can try it out

350
00:16:19,740 --> 00:16:25,770
today let me quickly talk about what

351
00:16:25,770 --> 00:16:28,829
mayor strategy does mirror strategy

352
00:16:28,829 --> 00:16:31,140
implements the synchronous all reduce

353
00:16:31,140 --> 00:16:33,690
architecture that we talked about out of

354
00:16:33,690 --> 00:16:38,070
the box for you in merge strategy the

355
00:16:38,070 --> 00:16:40,170
models parameters are mirrored across

356
00:16:40,170 --> 00:16:42,300
the various devices hence the name

357
00:16:42,300 --> 00:16:45,450
mirrored strategy each device computes

358
00:16:45,450 --> 00:16:48,420
the loss and gradients based on a subset

359
00:16:48,420 --> 00:16:51,750
of the input data the gradients are then

360
00:16:51,750 --> 00:16:54,600
aggregated across the workers using an

361
00:16:54,600 --> 00:16:57,180
all reduce algorithm that is appropriate

362
00:16:57,180 --> 00:17:02,370
for your device topology as I already

363
00:17:02,370 --> 00:17:04,350
mentioned with mirrored strategy you

364
00:17:04,350 --> 00:17:06,000
don't need to make any changes to your

365
00:17:06,000 --> 00:17:08,579
model or your training loop this is

366
00:17:08,579 --> 00:17:10,350
because we've changed underlying

367
00:17:10,350 --> 00:17:12,630
components of tensor flow to be

368
00:17:12,630 --> 00:17:15,959
distribution aware for example optimizer

369
00:17:15,959 --> 00:17:19,949
batch norm summaries etcetera you don't

370
00:17:19,949 --> 00:17:21,720
need to make any changes to your input

371
00:17:21,720 --> 00:17:23,670
function either as long as you're using

372
00:17:23,670 --> 00:17:27,740
the recommended tensor flow data set API

373
00:17:27,740 --> 00:17:29,760
saving and checkpointing works

374
00:17:29,760 --> 00:17:30,580
seamlessly

375
00:17:30,580 --> 00:17:33,220
so you can save with one or no

376
00:17:33,220 --> 00:17:35,440
distribution strategy and resume with

377
00:17:35,440 --> 00:17:38,620
another and summaries work as expected

378
00:17:38,620 --> 00:17:41,290
as well so you can continue to visualize

379
00:17:41,290 --> 00:17:45,010
your training intensive old mere

380
00:17:45,010 --> 00:17:46,660
strategy is just one type of

381
00:17:46,660 --> 00:17:49,060
distribution strategy and we're working

382
00:17:49,060 --> 00:17:51,280
on a few others for a variety of use

383
00:17:51,280 --> 00:17:55,540
cases I'll now hand it off to Anjali to

384
00:17:55,540 --> 00:17:57,910
show you some cool demos and performance

385
00:17:57,910 --> 00:18:04,620
numbers

386
00:18:04,620 --> 00:18:07,390
thanks Priya for the great introduction

387
00:18:07,390 --> 00:18:12,640
to Murad strategy before we run the demo

388
00:18:12,640 --> 00:18:14,680
let us get familiar with a few

389
00:18:14,680 --> 00:18:17,560
configurations I'm going to be running

390
00:18:17,560 --> 00:18:20,020
the resonate 50 model from the tensor

391
00:18:20,020 --> 00:18:23,170
flow model garden ResNet 50 is an

392
00:18:23,170 --> 00:18:26,160
emulsification model that has 50 layers

393
00:18:26,160 --> 00:18:29,290
it uses skip connections for efficient

394
00:18:29,290 --> 00:18:32,530
gradient flow the tensor flow model

395
00:18:32,530 --> 00:18:34,330
garden is a repo where there are

396
00:18:34,330 --> 00:18:36,460
collection of different models they're

397
00:18:36,460 --> 00:18:38,580
written in tensor flow high level ap is

398
00:18:38,580 --> 00:18:41,260
so if you are new to tensor flow this is

399
00:18:41,260 --> 00:18:44,410
a great resource to start with I'm going

400
00:18:44,410 --> 00:18:46,990
to be using the image net data set is

401
00:18:46,990 --> 00:18:50,110
input to model training the image net

402
00:18:50,110 --> 00:18:52,000
data set is a collection of over a

403
00:18:52,000 --> 00:18:53,680
million images that have been

404
00:18:53,680 --> 00:18:57,730
categorized into a thousand labels I'm

405
00:18:57,730 --> 00:19:00,010
going to instantiate the n1 standard

406
00:19:00,010 --> 00:19:03,970
instance on GCE and attach eight nvidia

407
00:19:03,970 --> 00:19:09,040
tesla v 100's or voltage GPUs let's run

408
00:19:09,040 --> 00:19:14,290
the demo now as I mentioned I'm creating

409
00:19:14,290 --> 00:19:18,400
an N 1 standard instance attaching 8

410
00:19:18,400 --> 00:19:22,390
Nvidia Tesla V 100 or voltage GPUs

411
00:19:22,390 --> 00:19:27,010
I also attach SSD disk this contains the

412
00:19:27,010 --> 00:19:29,740
image net data set which is input to our

413
00:19:29,740 --> 00:19:34,420
model training to run a tensor flow

414
00:19:34,420 --> 00:19:37,690
model we need to install a few drivers

415
00:19:37,690 --> 00:19:40,840
and pet packages and here is a gist with

416
00:19:40,840 --> 00:19:42,809
all the commands required

417
00:19:42,809 --> 00:19:45,179
I'm going to make this just public so

418
00:19:45,179 --> 00:19:47,279
you can set up an instance yourself and

419
00:19:47,279 --> 00:19:51,419
try running the model let's open an SSH

420
00:19:51,419 --> 00:19:53,909
connection to the instance by clicking

421
00:19:53,909 --> 00:19:58,649
on a button here this should bring up a

422
00:19:58,649 --> 00:19:59,909
terminal like this

423
00:19:59,909 --> 00:20:02,129
so I've already cloned the garden model

424
00:20:02,129 --> 00:20:04,590
repo we're going to be running this

425
00:20:04,590 --> 00:20:10,009
command inside the resonate directory

426
00:20:10,009 --> 00:20:12,179
we're going to run the image net main

427
00:20:12,179 --> 00:20:14,159
file so we're using the image net data

428
00:20:14,159 --> 00:20:19,999
set a bad-sized of 1024 or 128 per GPU a

429
00:20:19,999 --> 00:20:22,799
model directory is going to point to the

430
00:20:22,799 --> 00:20:24,629
GCS bucket that's going to hold our

431
00:20:24,629 --> 00:20:26,610
check points and summaries that we want

432
00:20:26,610 --> 00:20:29,879
to save we point our data directory to

433
00:20:29,879 --> 00:20:32,249
the SSD disk well charge the image net

434
00:20:32,249 --> 00:20:35,669
data set and the number of GPUs is 8

435
00:20:35,669 --> 00:20:37,740
over which we want to distribute or

436
00:20:37,740 --> 00:20:42,269
trade our model so let's run this model

437
00:20:42,269 --> 00:20:45,149
now and as the model is starting to

438
00:20:45,149 --> 00:20:48,029
train let's take a look at some of the

439
00:20:48,029 --> 00:20:51,360
code changes that are involved in the

440
00:20:51,360 --> 00:20:53,369
interchange the resinate model function

441
00:20:53,369 --> 00:20:56,159
so this is a resonate main function in

442
00:20:56,159 --> 00:20:59,610
the garden model repo first we

443
00:20:59,610 --> 00:21:02,720
instantiate the mirrored strategy object

444
00:21:02,720 --> 00:21:05,639
then we pass it to the run config as

445
00:21:05,639 --> 00:21:09,600
part of the Train distribute argument we

446
00:21:09,600 --> 00:21:12,149
create an estimator object with the run

447
00:21:12,149 --> 00:21:15,960
config and then we call trained on this

448
00:21:15,960 --> 00:21:18,419
estimator object and that's it those are

449
00:21:18,419 --> 00:21:19,980
all the code changes you need to

450
00:21:19,980 --> 00:21:23,850
distribute the ResNet model let's go

451
00:21:23,850 --> 00:21:28,039
back and see how our training is going

452
00:21:28,039 --> 00:21:30,629
so we've run out for a few hundred steps

453
00:21:30,629 --> 00:21:32,279
at the bottom of the screen you should

454
00:21:32,279 --> 00:21:35,580
see a few metrics the loss is descri

455
00:21:35,580 --> 00:21:38,840
cing steps per second learning rate

456
00:21:38,840 --> 00:21:41,519
let's look at tensor board so this is

457
00:21:41,519 --> 00:21:43,499
from a run where I've run the model for

458
00:21:43,499 --> 00:21:46,499
90,000 steps a little over that so it's

459
00:21:46,499 --> 00:21:49,289
not the run we just started so the

460
00:21:49,289 --> 00:21:51,149
orange and red lines are the training

461
00:21:51,149 --> 00:21:54,720
and evaluation losses so as a number of

462
00:21:54,720 --> 00:21:55,820
steps increase

463
00:21:55,820 --> 00:22:01,250
see the loss decreasing let's look at

464
00:22:01,250 --> 00:22:03,770
evaluation accuracy and this is when

465
00:22:03,770 --> 00:22:07,640
we're training ResNet 50 or 8 GPUs so if

466
00:22:07,640 --> 00:22:10,220
we see that around 91 thousand steps we

467
00:22:10,220 --> 00:22:11,690
were able to achieve a 75 percent

468
00:22:11,690 --> 00:22:17,150
accuracy let's see what it what this

469
00:22:17,150 --> 00:22:18,770
looks like when we run it on a single

470
00:22:18,770 --> 00:22:21,410
GPU so let's toggle the tensor board

471
00:22:21,410 --> 00:22:24,080
buttons on the left and look at the

472
00:22:24,080 --> 00:22:27,620
Train and evaluation loss curves when we

473
00:22:27,620 --> 00:22:29,420
train our model on one GPU so the blue

474
00:22:29,420 --> 00:22:31,970
lines are one CPU and red are orange and

475
00:22:31,970 --> 00:22:34,190
eight and you can see that the loss

476
00:22:34,190 --> 00:22:36,680
doesn't decrease as rapidly as it does

477
00:22:36,680 --> 00:22:39,640
with eight GPUs here are the evaluation

478
00:22:39,640 --> 00:22:42,680
accuracy curves were able to achieve a

479
00:22:42,680 --> 00:22:44,780
higher accuracy when we distribute our

480
00:22:44,780 --> 00:22:47,900
model across eight CPUs as opposed to

481
00:22:47,900 --> 00:22:48,410
one

482
00:22:48,410 --> 00:22:51,620
let's compare using wall time so we've

483
00:22:51,620 --> 00:22:53,990
run the same model for the same amount

484
00:22:53,990 --> 00:22:57,530
of time and when we run it over multiple

485
00:22:57,530 --> 00:22:59,600
GPUs we were able to achieve higher

486
00:22:59,600 --> 00:23:02,390
accuracy faster or trained our model

487
00:23:02,390 --> 00:23:10,370
faster let's look at a few performance

488
00:23:10,370 --> 00:23:14,900
benchmarks on the DG X 1 3 2 X 1 is a

489
00:23:14,900 --> 00:23:16,760
machine on win which on which we run

490
00:23:16,760 --> 00:23:18,190
deep learning models

491
00:23:18,190 --> 00:23:20,630
we're running miss mixed precision

492
00:23:20,630 --> 00:23:23,060
training with a per GPU batch size of

493
00:23:23,060 --> 00:23:29,930
256 it also has 8 volta or v 100 GPUs so

494
00:23:29,930 --> 00:23:33,710
the graph shows x-axis the number of

495
00:23:33,710 --> 00:23:36,200
GPUs on the x-axis and images per second

496
00:23:36,200 --> 00:23:39,890
on the y-axis so as we go from one GPU

497
00:23:39,890 --> 00:23:42,800
to a we are able to achieve a speed-up

498
00:23:42,800 --> 00:23:47,630
of 7x and this is performance right out

499
00:23:47,630 --> 00:23:51,170
of the box with no tuning we're actively

500
00:23:51,170 --> 00:23:53,450
working on improving performance so that

501
00:23:53,450 --> 00:23:55,820
you are able to achieve more speed up

502
00:23:55,820 --> 00:23:57,680
and get more images per second

503
00:23:57,680 --> 00:23:59,420
when you distribute your model across

504
00:23:59,420 --> 00:24:04,660
multiple GPUs

505
00:24:04,660 --> 00:24:07,150
so far we've been talking about the core

506
00:24:07,150 --> 00:24:09,790
part of model training and distributing

507
00:24:09,790 --> 00:24:13,270
your model using Murat strategy okay so

508
00:24:13,270 --> 00:24:15,190
let's say now you have deployed your

509
00:24:15,190 --> 00:24:17,770
model on multiple GPUs you're going to

510
00:24:17,770 --> 00:24:21,040
expect to see the same kind of boost in

511
00:24:21,040 --> 00:24:23,800
images per second when you do that but

512
00:24:23,800 --> 00:24:25,960
that may you may not be able to view as

513
00:24:25,960 --> 00:24:28,270
many images per second as compared to

514
00:24:28,270 --> 00:24:30,250
one CPU you may not see the boost and

515
00:24:30,250 --> 00:24:32,770
performance and the reason for that is

516
00:24:32,770 --> 00:24:39,190
often the input pipeline when you run

517
00:24:39,190 --> 00:24:41,950
your model on a single GPU the input

518
00:24:41,950 --> 00:24:44,470
pipeline is pre processing the data and

519
00:24:44,470 --> 00:24:46,810
making the data available on the GPU for

520
00:24:46,810 --> 00:24:50,680
training but GPUs or TP use as you know

521
00:24:50,680 --> 00:24:53,020
process and compute data much faster

522
00:24:53,020 --> 00:24:56,890
than a CPU this means that when you

523
00:24:56,890 --> 00:24:58,930
distribute your model across multiple

524
00:24:58,930 --> 00:25:02,140
GPUs the input pipeline is also not able

525
00:25:02,140 --> 00:25:04,900
to keep up with the training it quickly

526
00:25:04,900 --> 00:25:09,850
becomes a bottleneck for the rest of the

527
00:25:09,850 --> 00:25:11,740
talk I'm going to show you how

528
00:25:11,740 --> 00:25:14,010
tensorflow makes it easy for you to use

529
00:25:14,010 --> 00:25:17,740
TF the data API is to build efficient

530
00:25:17,740 --> 00:25:25,900
and performant input pipelines here's a

531
00:25:25,900 --> 00:25:29,340
simple input pipeline for ResNet 50

532
00:25:29,340 --> 00:25:32,260
we're going to use TF da-ta-da api's

533
00:25:32,260 --> 00:25:34,930
because data sets are awesome they

534
00:25:34,930 --> 00:25:37,330
helped us build complex pipeline using

535
00:25:37,330 --> 00:25:40,810
simple reusable pieces when you have

536
00:25:40,810 --> 00:25:43,480
lots of data and different data formats

537
00:25:43,480 --> 00:25:45,310
and you want to perform complex

538
00:25:45,310 --> 00:25:47,830
transformations on this data you want to

539
00:25:47,830 --> 00:25:50,170
be using T of data API is to build your

540
00:25:50,170 --> 00:25:54,670
input pipeline first we are going to use

541
00:25:54,670 --> 00:25:57,640
the list files API to get the list of

542
00:25:57,640 --> 00:26:00,580
input files that contain your image and

543
00:26:00,580 --> 00:26:03,550
labels then we are going to read these

544
00:26:03,550 --> 00:26:05,800
files using the TF record data set

545
00:26:05,800 --> 00:26:08,740
reader we're going to shuffle the

546
00:26:08,740 --> 00:26:12,190
records repeat them a few times

547
00:26:12,190 --> 00:26:13,810
depending on if you want to run your

548
00:26:13,810 --> 00:26:17,470
model for a couple of epochs and finally

549
00:26:17,470 --> 00:26:18,509
apply your map

550
00:26:18,509 --> 00:26:22,169
permission so this processes each record

551
00:26:22,169 --> 00:26:25,019
and applies a transformation such as

552
00:26:25,019 --> 00:26:27,889
cropping flipping image decoding and

553
00:26:27,889 --> 00:26:31,649
finally batch the input and finally

554
00:26:31,649 --> 00:26:34,589
batch the input into a batch size that

555
00:26:34,589 --> 00:26:41,070
you desire the input pipeline can be

556
00:26:41,070 --> 00:26:44,369
thought of as an ETL process which is

557
00:26:44,369 --> 00:26:50,099
extract transform and load process in

558
00:26:50,099 --> 00:26:52,559
the extract phase we are reading from

559
00:26:52,559 --> 00:26:55,259
persistent storage which can be local or

560
00:26:55,259 --> 00:26:59,249
remote in the transform phase we are

561
00:26:59,249 --> 00:27:01,139
applying the different transformations

562
00:27:01,139 --> 00:27:05,449
like shuffled repeat map and batch and

563
00:27:05,449 --> 00:27:08,129
finally in the load phase we are

564
00:27:08,129 --> 00:27:10,409
providing this processed data to the

565
00:27:10,409 --> 00:27:13,709
accelerator for training so how does

566
00:27:13,709 --> 00:27:15,690
this apply to the example that we just

567
00:27:15,690 --> 00:27:22,409
saw in the extract phase we list the

568
00:27:22,409 --> 00:27:25,259
files and read it using the TF record

569
00:27:25,259 --> 00:27:28,699
data set reader in the transform phase

570
00:27:28,699 --> 00:27:32,279
we apply the shuffle repeat map and

571
00:27:32,279 --> 00:27:35,489
batch transformations and finally in the

572
00:27:35,489 --> 00:27:37,919
load phase we tell sensor flow how to

573
00:27:37,919 --> 00:27:44,669
grab the data from the data set this is

574
00:27:44,669 --> 00:27:47,579
what our input pipeline looks like we

575
00:27:47,579 --> 00:27:49,440
have the extract transform and load

576
00:27:49,440 --> 00:27:52,709
phases happening sequentially followed

577
00:27:52,709 --> 00:27:56,099
by the training on the accelerator this

578
00:27:56,099 --> 00:27:57,989
means whether when the CPU is busy

579
00:27:57,989 --> 00:28:00,899
pre-processing the data the accelerator

580
00:28:00,899 --> 00:28:03,690
is idle and where the accelerator is

581
00:28:03,690 --> 00:28:08,039
training your model the CPU is idle but

582
00:28:08,039 --> 00:28:10,440
the different phases of the ETL process

583
00:28:10,440 --> 00:28:13,440
use different hardware resources for

584
00:28:13,440 --> 00:28:16,259
example the extract step uses the

585
00:28:16,259 --> 00:28:19,289
persistent storage the transform step

586
00:28:19,289 --> 00:28:21,739
uses a different course of the CPU and

587
00:28:21,739 --> 00:28:24,449
finally the training happens on the

588
00:28:24,449 --> 00:28:28,349
accelerator so if we can paralyze these

589
00:28:28,349 --> 00:28:31,349
different phases then we can overlap the

590
00:28:31,349 --> 00:28:31,690
pre

591
00:28:31,690 --> 00:28:34,360
processing of data on the CPU with

592
00:28:34,360 --> 00:28:38,889
training of the model on the GPU this is

593
00:28:38,889 --> 00:28:42,309
called pipelining so we can use

594
00:28:42,309 --> 00:28:44,830
pipelining and some parallelization

595
00:28:44,830 --> 00:28:47,139
techniques to build more efficient

596
00:28:47,139 --> 00:28:50,769
import pipelines let us look at few of

597
00:28:50,769 --> 00:28:55,049
these techniques

598
00:28:55,049 --> 00:28:57,970
first you can parallelize file reading

599
00:28:57,970 --> 00:29:00,399
let's say you have a lot of data that

600
00:29:00,399 --> 00:29:02,620
sharded a car across a cloud storage

601
00:29:02,620 --> 00:29:06,370
service you want read multiple files and

602
00:29:06,370 --> 00:29:09,460
parallel and you can do this using the

603
00:29:09,460 --> 00:29:11,980
non parallel reads call as when you

604
00:29:11,980 --> 00:29:14,080
instantiate the Tierra when you call a

605
00:29:14,080 --> 00:29:18,820
TF record data set API this allows you

606
00:29:18,820 --> 00:29:24,340
to increase your effective throughput we

607
00:29:24,340 --> 00:29:26,799
can also paralyze map function for

608
00:29:26,799 --> 00:29:30,220
transformations you can data paralyze

609
00:29:30,220 --> 00:29:33,610
the different transformation for of the

610
00:29:33,610 --> 00:29:37,740
map function by using the numpad L calls

611
00:29:37,740 --> 00:29:42,970
argument typically the num the argument

612
00:29:42,970 --> 00:29:45,190
we provide is a number of cores of the

613
00:29:45,190 --> 00:29:52,720
CPU and finally you want to call

614
00:29:52,720 --> 00:29:55,360
prefetch at the end of your input

615
00:29:55,360 --> 00:29:58,899
pipeline prefetch decouples the time the

616
00:29:58,899 --> 00:30:01,149
data is produced from the time it is

617
00:30:01,149 --> 00:30:04,210
consumed this means that you can buffer

618
00:30:04,210 --> 00:30:06,610
data for the next training step while

619
00:30:06,610 --> 00:30:09,039
the accelerator is still training the

620
00:30:09,039 --> 00:30:16,480
current step this is what we had before

621
00:30:16,480 --> 00:30:19,670
and this is what we can get an

622
00:30:19,670 --> 00:30:23,660
improvement on here the different phases

623
00:30:23,660 --> 00:30:26,060
of the input pipeline are happening in

624
00:30:26,060 --> 00:30:29,450
parallel were training we are able to

625
00:30:29,450 --> 00:30:31,760
see that the CPU is pre processing data

626
00:30:31,760 --> 00:30:33,620
for the training step 2

627
00:30:33,620 --> 00:30:35,450
while the accelerator is still training

628
00:30:35,450 --> 00:30:39,410
step 1 neither the CPU now the

629
00:30:39,410 --> 00:30:41,870
accelerator is idle for long periods of

630
00:30:41,870 --> 00:30:45,770
time the training time is now a maximum

631
00:30:45,770 --> 00:30:48,860
of pre processing and training on the

632
00:30:48,860 --> 00:30:52,640
accelerator as you can see the

633
00:30:52,640 --> 00:30:54,620
accelerator is still or not 100%

634
00:30:54,620 --> 00:30:57,800
utilized there are few advanced

635
00:30:57,800 --> 00:30:59,900
techniques that we can add to our input

636
00:30:59,900 --> 00:31:07,430
pipeline to improve this we can use fuse

637
00:31:07,430 --> 00:31:08,810
transformation ops

638
00:31:08,810 --> 00:31:13,220
of some of these API calls shuffle and

639
00:31:13,220 --> 00:31:17,210
repeat for example can be replaced by

640
00:31:17,210 --> 00:31:19,930
its equivalent fused up

641
00:31:19,930 --> 00:31:24,260
so this paralyzes buffering elements for

642
00:31:24,260 --> 00:31:26,990
epoch n plus 1 while producing elements

643
00:31:26,990 --> 00:31:33,560
for epoch n we can also replace map and

644
00:31:33,560 --> 00:31:37,550
batch with its equivalent fused up this

645
00:31:37,550 --> 00:31:39,830
paralyzes paralyzes the map

646
00:31:39,830 --> 00:31:43,610
transformation with adding the input

647
00:31:43,610 --> 00:31:50,000
tensors to batch with these techniques

648
00:31:50,000 --> 00:31:52,910
we are able to process data much faster

649
00:31:52,910 --> 00:31:54,830
and make it available to the accelerator

650
00:31:54,830 --> 00:31:58,130
for training and improve the training

651
00:31:58,130 --> 00:32:04,790
speed

652
00:32:04,790 --> 00:32:07,730
I hope this gives you a good idea of how

653
00:32:07,730 --> 00:32:10,370
you can use TF data API is to build

654
00:32:10,370 --> 00:32:13,310
efficient and performant input pipelines

655
00:32:13,310 --> 00:32:19,910
when you train your model so far we've

656
00:32:19,910 --> 00:32:22,100
been talking about training on a single

657
00:32:22,100 --> 00:32:24,770
machine and multiple devices but what if

658
00:32:24,770 --> 00:32:26,950
you wanted to train on multiple machines

659
00:32:26,950 --> 00:32:30,260
you can use as the estimators train and

660
00:32:30,260 --> 00:32:34,910
evaluate API train and evaluate API uses

661
00:32:34,910 --> 00:32:39,860
the acing parameter server approach this

662
00:32:39,860 --> 00:32:43,490
API is used widely within Google and it

663
00:32:43,490 --> 00:32:45,230
scales well to a large number of

664
00:32:45,230 --> 00:32:50,630
machines here's a link to the API where

665
00:32:50,630 --> 00:32:59,460
you can learn more on how to use it

666
00:32:59,460 --> 00:33:01,859
we're also excited to be working on a

667
00:33:01,859 --> 00:33:04,549
number of new distribution strategies

668
00:33:04,549 --> 00:33:07,349
we're working on a multi machine Mirage

669
00:33:07,349 --> 00:33:09,389
strategy which allows you to distribute

670
00:33:09,389 --> 00:33:11,849
your module across many machines with

671
00:33:11,849 --> 00:33:15,719
many devices we're also working on

672
00:33:15,719 --> 00:33:18,239
adding distribution strategy support to

673
00:33:18,239 --> 00:33:24,779
TP use and directly in TF dot chaos in

674
00:33:24,779 --> 00:33:27,239
this talk we've talked a lot about the

675
00:33:27,239 --> 00:33:28,589
different concepts related to

676
00:33:28,589 --> 00:33:30,599
distributor training architectures in

677
00:33:30,599 --> 00:33:35,129
API but when you go home today here are

678
00:33:35,129 --> 00:33:36,509
three things for you to keep in mind

679
00:33:36,509 --> 00:33:42,749
when you train your model distribute

680
00:33:42,749 --> 00:33:46,979
your training to make it faster to do

681
00:33:46,979 --> 00:33:50,159
this you want to use distribution

682
00:33:50,159 --> 00:33:53,369
strategy api's they're easy to use and

683
00:33:53,369 --> 00:33:58,919
they're fast input pipeline performance

684
00:33:58,919 --> 00:34:00,349
is important

685
00:34:00,349 --> 00:34:04,229
use TF data api is to build efficient

686
00:34:04,229 --> 00:34:10,829
input pipelines here are a few tenths of

687
00:34:10,829 --> 00:34:15,450
flow resources first we have the

688
00:34:15,450 --> 00:34:18,929
distribution strategy API you can try

689
00:34:18,929 --> 00:34:21,029
using mirrored strategy to train your

690
00:34:21,029 --> 00:34:27,599
model across multiple GPUs here's a link

691
00:34:27,599 --> 00:34:30,210
to the resonate 50 model garden example

692
00:34:30,210 --> 00:34:32,669
so you can try running this example it

693
00:34:32,669 --> 00:34:34,740
has mirrored strategy API support

694
00:34:34,740 --> 00:34:39,869
enabled here's a link also to the input

695
00:34:39,869 --> 00:34:42,210
pipeline performance guide which has

696
00:34:42,210 --> 00:34:44,549
more techniques that you can use to

697
00:34:44,549 --> 00:34:49,279
build efficient input pipelines and

698
00:34:49,279 --> 00:34:52,319
here's the link to the gist that I

699
00:34:52,319 --> 00:34:54,929
mentioned in the demo you can try

700
00:34:54,929 --> 00:34:57,149
setting up your own instance on running

701
00:34:57,149 --> 00:35:00,020
the resonate 50 model garden example

702
00:35:00,020 --> 00:35:02,910
thank you for attending our talk and we

703
00:35:02,910 --> 00:35:05,040
hope you had a great IO

704
00:35:05,040 --> 00:35:26,339
[Music]

