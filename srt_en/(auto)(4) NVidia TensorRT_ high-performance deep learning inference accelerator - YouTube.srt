1
00:00:03,020 --> 00:00:05,220
hi everybody and welcome to this episode

2
00:00:05,220 --> 00:00:07,980
of tensorflow meets on this episode it's

3
00:00:07,980 --> 00:00:09,570
my privilege to have Chris Scott breath

4
00:00:09,570 --> 00:00:12,090
from Nvidia and XQ from right here in

5
00:00:12,090 --> 00:00:14,670
Google to talk about tensor RT now

6
00:00:14,670 --> 00:00:16,560
tensor RT Chris tell us all about it

7
00:00:16,560 --> 00:00:18,720
tensor T is our programmable inference

8
00:00:18,720 --> 00:00:20,640
accelerator it's basically software that

9
00:00:20,640 --> 00:00:22,710
we created to allow people who are

10
00:00:22,710 --> 00:00:25,080
creating neural networks and artificial

11
00:00:25,080 --> 00:00:28,099
intelligence to run those networks in

12
00:00:28,099 --> 00:00:31,080
production or in devices with the full

13
00:00:31,080 --> 00:00:34,500
performance that GPUs can offer a lot of

14
00:00:34,500 --> 00:00:35,700
the software that's developed for

15
00:00:35,700 --> 00:00:37,469
artificial intelligence is designed for

16
00:00:37,469 --> 00:00:39,329
training and there's optimization so we

17
00:00:39,329 --> 00:00:41,100
can apply for inference that can get

18
00:00:41,100 --> 00:00:42,719
even more performance that aren't

19
00:00:42,719 --> 00:00:46,350
appropriate for for training again we

20
00:00:46,350 --> 00:00:49,860
created a tensor R T to accomplish this

21
00:00:49,860 --> 00:00:53,340
sort of goal of performance because

22
00:00:53,340 --> 00:00:55,550
that's what GPU you you come to GPUs for

23
00:00:55,550 --> 00:00:57,780
and because we're going to be using it

24
00:00:57,780 --> 00:01:00,180
in inference we also wanted to focus on

25
00:01:00,180 --> 00:01:01,620
robustness so we created as a shared

26
00:01:01,620 --> 00:01:03,780
library it's a very compact modular eyes

27
00:01:03,780 --> 00:01:06,500
thing that you can build on top of it

28
00:01:06,500 --> 00:01:08,130
kind of what we've done with the

29
00:01:08,130 --> 00:01:10,470
tensorflow integration but can also be

30
00:01:10,470 --> 00:01:14,520
used in things like cars and other other

31
00:01:14,520 --> 00:01:16,170
sort of devices so it's not just good

32
00:01:16,170 --> 00:01:17,850
for like tensor RT and NVIDIA right it's

33
00:01:17,850 --> 00:01:19,619
also been great for a tensor flow for us

34
00:01:19,619 --> 00:01:20,729
to be able to work with you could you

35
00:01:20,729 --> 00:01:22,830
tell us all about it excuse T so flow is

36
00:01:22,830 --> 00:01:25,049
a machine learning system as we are

37
00:01:25,049 --> 00:01:27,119
improving our inference time

38
00:01:27,119 --> 00:01:29,939
performances and we really want to give

39
00:01:29,939 --> 00:01:32,820
our users the best experience where's

40
00:01:32,820 --> 00:01:35,939
Tessa Artie and so we really want the

41
00:01:35,939 --> 00:01:38,130
user to within the error without

42
00:01:38,130 --> 00:01:40,350
stepping outside the tester flow within

43
00:01:40,350 --> 00:01:43,530
their inner development cycles be able

44
00:01:43,530 --> 00:01:47,040
to enjoy the benefit of the tensor RT

45
00:01:47,040 --> 00:01:49,500
and just have automatically have the

46
00:01:49,500 --> 00:01:52,619
performance benefit without much trouble

47
00:01:52,619 --> 00:01:54,509
it's easy for them to try and it see

48
00:01:54,509 --> 00:01:56,939
them to finding their models for better

49
00:01:56,939 --> 00:01:59,520
performance so we come to this design

50
00:01:59,520 --> 00:02:01,770
that basically give the best of the both

51
00:02:01,770 --> 00:02:04,409
world to the developers so they can

52
00:02:04,409 --> 00:02:06,689
enjoy the full diversity and the

53
00:02:06,689 --> 00:02:09,149
flexibility of tensor flow while still

54
00:02:09,149 --> 00:02:12,750
enjoy the performance benefit of tester

55
00:02:12,750 --> 00:02:13,590
Artie so

56
00:02:13,590 --> 00:02:15,930
come with the design and the start up on

57
00:02:15,930 --> 00:02:17,849
the discussing the cassava cooperation

58
00:02:17,849 --> 00:02:19,590
with a media it's a it's actually a

59
00:02:19,590 --> 00:02:22,290
really cool that you the that idea

60
00:02:22,290 --> 00:02:24,269
because the original sort of thought

61
00:02:24,269 --> 00:02:26,430
that we had for for ten sortie was to

62
00:02:26,430 --> 00:02:28,170
think of as a post processing step and I

63
00:02:28,170 --> 00:02:30,239
think it's a cool idea to really bring

64
00:02:30,239 --> 00:02:32,280
it into the development environment then

65
00:02:32,280 --> 00:02:34,920
as we've talked to some of the early

66
00:02:34,920 --> 00:02:36,660
folks who are who are looking at this

67
00:02:36,660 --> 00:02:38,670
they really liked that that change it's

68
00:02:38,670 --> 00:02:40,590
a very positive change for users on so

69
00:02:40,590 --> 00:02:43,410
how did this all get started well I mean

70
00:02:43,410 --> 00:02:46,890
users have been coming to us who are

71
00:02:46,890 --> 00:02:50,549
starting with tensorflow you know really

72
00:02:50,549 --> 00:02:52,140
from from the very beginning with with

73
00:02:52,140 --> 00:02:53,760
ten sororities so we knew that there was

74
00:02:53,760 --> 00:02:55,620
important community to engage with and

75
00:02:55,620 --> 00:02:57,930
NVIDIA and Google had been working

76
00:02:57,930 --> 00:03:00,269
together for a long time for GPU

77
00:03:00,269 --> 00:03:02,099
optimization on intensify so there was

78
00:03:02,099 --> 00:03:04,349
already an existing working relationship

79
00:03:04,349 --> 00:03:06,959
and I think you actually you just you

80
00:03:06,959 --> 00:03:10,500
suggested early on with an architecture

81
00:03:10,500 --> 00:03:12,239
quick little white paper hey it wouldn't

82
00:03:12,239 --> 00:03:16,349
be cool if and and we noodle on it and

83
00:03:16,349 --> 00:03:17,250
we thought actually sounded like a

84
00:03:17,250 --> 00:03:19,230
really good idea and got engaged do you

85
00:03:19,230 --> 00:03:20,790
still have that white paper have you

86
00:03:20,790 --> 00:03:23,910
framed it some of its on disk somewhere

87
00:03:23,910 --> 00:03:26,160
it's in Google Drive I think actually ok

88
00:03:26,160 --> 00:03:29,670
so it kept securely exactly so I'm a

89
00:03:29,670 --> 00:03:31,709
developer and I want to start using this

90
00:03:31,709 --> 00:03:33,389
I want to take advantage of this what do

91
00:03:33,389 --> 00:03:35,459
i do how do I get started so well you

92
00:03:35,459 --> 00:03:37,799
start in intensive flow itself so you

93
00:03:37,799 --> 00:03:40,049
you've created a graph in tensor flow

94
00:03:40,049 --> 00:03:44,579
and then with the capabilities that have

95
00:03:44,579 --> 00:03:46,919
been added through this integration I

96
00:03:46,919 --> 00:03:49,049
think you start with tips or flow of 1.7

97
00:03:49,049 --> 00:03:52,739
yes and you'll have your graph you'll

98
00:03:52,739 --> 00:03:56,220
you'll be tuning it as as xq said you'll

99
00:03:56,220 --> 00:03:57,359
get to the point real okay now now

100
00:03:57,359 --> 00:03:59,459
what's my performance like on a V 100 so

101
00:03:59,459 --> 00:04:00,780
you'll you'll you'll go on a machine

102
00:04:00,780 --> 00:04:03,900
that has a V 100 and you'll save the

103
00:04:03,900 --> 00:04:05,250
graph which I think means freezing

104
00:04:05,250 --> 00:04:06,540
that's the freezing is what we

105
00:04:06,540 --> 00:04:07,079
tensorflow

106
00:04:07,079 --> 00:04:10,349
ATM for for saving it so you're still in

107
00:04:10,349 --> 00:04:11,730
tends to flow but now you have the the

108
00:04:11,730 --> 00:04:13,799
graph set up to do inference then

109
00:04:13,799 --> 00:04:15,630
there's a command that was added that

110
00:04:15,630 --> 00:04:18,120
will allow you to do a graph transform

111
00:04:18,120 --> 00:04:21,090
okay and the graph transform will take

112
00:04:21,090 --> 00:04:23,099
the take the network and is ex Cuse said

113
00:04:23,099 --> 00:04:25,380
it'll find the places the subgraph of

114
00:04:25,380 --> 00:04:26,760
the whole graph

115
00:04:26,760 --> 00:04:29,130
that tensor arty can handle and it'll

116
00:04:29,130 --> 00:04:31,350
actually convert that into I think a

117
00:04:31,350 --> 00:04:34,470
single kind of tensor arty op so you can

118
00:04:34,470 --> 00:04:36,810
still look at it with with tensor board

119
00:04:36,810 --> 00:04:37,800
or something like that and you'll just

120
00:04:37,800 --> 00:04:40,140
see that you know the data input it'll

121
00:04:40,140 --> 00:04:42,000
go into the tensor RT op and then you'll

122
00:04:42,000 --> 00:04:43,680
get whatever post-processing you you

123
00:04:43,680 --> 00:04:47,970
might do and the cool thing is that's a

124
00:04:47,970 --> 00:04:49,620
graph right so you're still in tensor

125
00:04:49,620 --> 00:04:52,170
flow and all the things you can normally

126
00:04:52,170 --> 00:04:53,070
do with graphs you can do in

127
00:04:53,070 --> 00:04:54,270
particularly you can you can run in

128
00:04:54,270 --> 00:04:55,770
prints on it right there okay where you

129
00:04:55,770 --> 00:04:57,090
can save it and then run it you know in

130
00:04:57,090 --> 00:04:58,320
cancer flow surveying or something like

131
00:04:58,320 --> 00:04:59,610
that later on cool

132
00:04:59,610 --> 00:05:01,380
I'm we're only really getting started

133
00:05:01,380 --> 00:05:03,990
right yes so this is a nice approach

134
00:05:03,990 --> 00:05:07,020
about this this is the first step toward

135
00:05:07,020 --> 00:05:08,940
the grand vision and the design that we

136
00:05:08,940 --> 00:05:10,920
have in mind right in the current form

137
00:05:10,920 --> 00:05:13,020
it is very interesting that you can

138
00:05:13,020 --> 00:05:13,890
steal the user

139
00:05:13,890 --> 00:05:16,560
everything happens with integer flow and

140
00:05:16,560 --> 00:05:19,890
the users can use a full flexibility and

141
00:05:19,890 --> 00:05:22,800
the generality of the ops that offered

142
00:05:22,800 --> 00:05:24,690
by tensor flow in whatever week they

143
00:05:24,690 --> 00:05:27,030
want the contributed the graph to the

144
00:05:27,030 --> 00:05:29,760
hearts and ten and still for the sub

145
00:05:29,760 --> 00:05:32,070
graph that the tensor RT can support

146
00:05:32,070 --> 00:05:34,650
they will give them the performance

147
00:05:34,650 --> 00:05:36,000
benefit they would you so how do they

148
00:05:36,000 --> 00:05:38,070
then do basically that the in the

149
00:05:38,070 --> 00:05:39,270
current form you still need to do a

150
00:05:39,270 --> 00:05:41,970
graph transformation and in the next

151
00:05:41,970 --> 00:05:43,830
view in the future well our teams who

152
00:05:43,830 --> 00:05:45,780
kind of work very close together so you

153
00:05:45,780 --> 00:05:47,700
don't even have to create a new graph it

154
00:05:47,700 --> 00:05:49,560
just good crea graph and the mention

155
00:05:49,560 --> 00:05:51,870
that I want to enable this RT now and

156
00:05:51,870 --> 00:05:54,330
which is magically get a performance

157
00:05:54,330 --> 00:05:56,130
improvement from the front that SRT

158
00:05:56,130 --> 00:05:59,160
still enjoy the natural development

159
00:05:59,160 --> 00:06:02,310
experience way is tensor flow and that

160
00:06:02,310 --> 00:06:03,330
was one of the things actually isn't it

161
00:06:03,330 --> 00:06:05,370
an Nvidia your processors you've run on

162
00:06:05,370 --> 00:06:06,660
multiple environments right there's auto

163
00:06:06,660 --> 00:06:08,220
mode Evan that's embedded in this cloud

164
00:06:08,220 --> 00:06:10,110
and and they just work across all of

165
00:06:10,110 --> 00:06:12,150
them yeah yeah and it's also supplies

166
00:06:12,150 --> 00:06:13,770
the yes it does

167
00:06:13,770 --> 00:06:16,560
and it streamlines the the data flow so

168
00:06:16,560 --> 00:06:18,060
one of the things that previous to this

169
00:06:18,060 --> 00:06:20,910
work we had customers and we would work

170
00:06:20,910 --> 00:06:22,530
and help them through through this

171
00:06:22,530 --> 00:06:25,590
challenges that it is taking the the

172
00:06:25,590 --> 00:06:28,980
data they could export the the tensor

173
00:06:28,980 --> 00:06:31,080
flow graph in a tense RT but in

174
00:06:31,080 --> 00:06:32,670
intensive flow they were probably doing

175
00:06:32,670 --> 00:06:34,050
some pre-processing and probably you say

176
00:06:34,050 --> 00:06:35,460
who's doing some post-processing using

177
00:06:35,460 --> 00:06:37,380
the tools that a tensor flow environment

178
00:06:37,380 --> 00:06:39,390
brings to them they would have to kind

179
00:06:39,390 --> 00:06:40,590
of replace that with with

180
00:06:40,590 --> 00:06:44,010
see code and and if they can you know if

181
00:06:44,010 --> 00:06:45,120
they can take advantage of the

182
00:06:45,120 --> 00:06:46,350
integration then they can just use the

183
00:06:46,350 --> 00:06:47,730
existing code they've already written in

184
00:06:47,730 --> 00:06:49,110
history whoa so reducing a lot of

185
00:06:49,110 --> 00:06:50,790
friction but yeah I'm able to access all

186
00:06:50,790 --> 00:06:52,470
and then being able to yeah again again

187
00:06:52,470 --> 00:06:55,530
to apply the graph or the artificial

188
00:06:55,530 --> 00:06:56,940
intelligence your Mottola they've

189
00:06:56,940 --> 00:06:58,410
created they can put it in a drone or

190
00:06:58,410 --> 00:07:01,440
they can put it in a in a car or in the

191
00:07:01,440 --> 00:07:03,060
cloud cool cool

192
00:07:03,060 --> 00:07:05,100
so again as a developer there's a

193
00:07:05,100 --> 00:07:07,110
there's a site on NVIDIA right for

194
00:07:07,110 --> 00:07:11,060
developers yeah we'll have the URL is

195
00:07:11,060 --> 00:07:18,479
HTTP slash but surely yeah so yeah it's

196
00:07:18,479 --> 00:07:20,070
basically our developer site has a tense

197
00:07:20,070 --> 00:07:22,020
arty page okay cool so we'll put the

198
00:07:22,020 --> 00:07:23,790
link in the description below so if

199
00:07:23,790 --> 00:07:25,290
you're a developer you go visit that

200
00:07:25,290 --> 00:07:26,760
link and you can learn about all about

201
00:07:26,760 --> 00:07:28,020
tense Rorty and work through some

202
00:07:28,020 --> 00:07:30,150
scenarios you'll be able to add but you

203
00:07:30,150 --> 00:07:31,440
can you can get information there and

204
00:07:31,440 --> 00:07:33,960
it's also just available in tensorflow I

205
00:07:33,960 --> 00:07:35,669
mean so yeah so when you get ten zero

206
00:07:35,669 --> 00:07:38,340
one point seven yeah be there okay I'm

207
00:07:38,340 --> 00:07:40,050
on that site every couple of weeks

208
00:07:40,050 --> 00:07:42,030
downloading my q2 drivers anyway so I'll

209
00:07:42,030 --> 00:07:45,690
check it out thank you so much Chris and

210
00:07:45,690 --> 00:07:47,700
thank you so much XQ and thanks

211
00:07:47,700 --> 00:07:49,229
everybody for watching this episode of

212
00:07:49,229 --> 00:07:50,760
tensor flow mates if you've got any

213
00:07:50,760 --> 00:07:52,860
questions for me or any questions for my

214
00:07:52,860 --> 00:07:54,030
guests just please leave them in the

215
00:07:54,030 --> 00:07:55,590
comments below and we hope to see you in

216
00:07:55,590 --> 00:08:01,920
a future episode thank you

217
00:08:01,920 --> 00:08:03,980
you

